{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc36f561-1f9c-4675-8c90-116ada842e5f",
   "metadata": {},
   "source": [
    "# Mensajes en LangChain\n",
    "\n",
    "En LangChain, los mensajes son objetos que encapsulan información en las interacciones con los modelos, y no simples cadenas de texto.\n",
    "Esto permite manejar roles, metadatos y control de conversación de forma estructurada.\n",
    "\n",
    "## Tipos de Mensajes\n",
    "\n",
    "LangChain define un **formato unificado de mensajes** para todos los modelos de chat, permitiendo trabajar con distintos proveedores sin preocuparse por diferencias en sus formatos.\n",
    "\n",
    "Todos los mensajes son objetos de Python que heredan de `BaseMessage`.\n",
    "\n",
    "### Tipos principales\n",
    "- **`SystemMessage`** → Rol de sistema (define comportamiento o contexto global del asistente).\n",
    "- **`HumanMessage`** → Rol de usuario (entrada proveniente del humano).\n",
    "- **`AIMessage`** → Rol de asistente (respuesta generada por el modelo).\n",
    "- **`AIMessageChunk`** → Rol de asistente, usado para *streaming* de respuestas parciales.\n",
    "- **`ToolMessage`** → Rol de herramienta (respuestas de herramientas invocadas por el modelo).\n",
    "\n",
    "### Otros mensajes relevantes\n",
    "- **`RemoveMessage`** → Sin rol asignado; se usa principalmente en **LangGraph** para gestionar historial de chat.\n",
    "- **`FunctionMessage`** *(legado)* → Rol de función en la API de llamadas a funciones de OpenAI (versión anterior).\n",
    "\n",
    "Para más detalles, consulta la [API Reference](https://python.langchain.com/api_reference/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd643c-546b-45d0-a4cc-d009b0c7f4c8",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. HumanMessage\n",
    "El HumanMessage representa una intervención realizada por un usuario humano. Es el mensaje que usualmente inicia la conversación o una nueva instrucción. Se utiliza para formular preguntas, dar comandos o realizar solicitudes al modelo.\n",
    "\n",
    "Propósito:\n",
    "- Representa lo que el usuario desea comunicar al modelo.\n",
    "-  Es el input principal que desencadena una respuesta del modelo\n",
    "\n",
    "\n",
    "#### **Ejemplo de uso:**\n",
    "```python\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "mensaje_usuario = HumanMessage(\n",
    "    content=\"¿Puedes explicarme cómo resolver una ecuación cuadrática paso a paso?\"\n",
    ")\n",
    "```\n",
    "\n",
    "### **2. `SystemMessage`**\n",
    "Un `SystemMessage` es un mensaje utilizado para definir el **contexto** o las **instrucciones iniciales** que guiarán al modelo durante toda la interacción. Se utiliza para establecer el comportamiento esperado del modelo o proporcionar un marco de trabajo para el resto de la conversación.\n",
    "\n",
    "#### **Propósito:**\n",
    "- Configurar el tono, los límites y las expectativas del modelo.\n",
    "- Asegurarse de que el modelo comprenda el propósito principal de la interacción.\n",
    "- Proveer instrucciones claras y precisas que influirán en cómo se interpretan y generan las respuestas.\n",
    "\n",
    "#### **Ejemplo de uso:**\n",
    "```python\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "mensaje_sistema = SystemMessage(\n",
    "    content=\"Eres un asistente experto en matemáticas que ayuda a resolver problemas paso a paso.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84135840-234f-47a5-81a0-45332d93dc12",
   "metadata": {},
   "source": [
    "### 3. `AIMessage`\n",
    "El `AIMessage` representa la respuesta generada por el modelo. Contiene el contenido producido tras procesar uno o más mensajes anteriores.\n",
    "\n",
    "**Propósito:**\n",
    "- Entregar la respuesta generada por la IA.\n",
    "- Mantener el historial de interacciones con el rol de asistente.\n",
    "\n",
    "#### **Ejemplo de uso:**\n",
    "```python\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "mensaje_ai = AIMessage(\n",
    "    content=\"Para resolver una ecuación cuadrática, primero identifica los coeficientes a, b y c...\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a69f4d-9b8d-4d10-ae82-2f6fc8d2c8bc",
   "metadata": {},
   "source": [
    "### Importación de las librerías\n",
    "- Para poder usar las clases, se realizan las importaciones relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3b838-02e6-4abe-8e30-479b017a45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd6eeaf-f879-4e05-8a11-346894027581",
   "metadata": {},
   "source": [
    "### Conexion mediante el modelo de OpenAI"
   ]
  },
  {
   "cell_type": "raw",
   "id": "43d8ea1c-933e-4529-9cd8-7b4348ffc002",
   "metadata": {},
   "source": [
    "# Habilite como code, y dessabilite la instanciacion de los otros modelos, dejandolo en modo raw\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env (busca automáticamente en el directorio actual o superiores)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró OPENAI_API_KEY en el .env ni en el entorno\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),   # sk-proj-...\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a8855-87d6-481b-b1dd-b2079aa4b097",
   "metadata": {},
   "source": [
    "### Conexion mediante modelos de Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32846e6-7d0d-4311-9fbc-5076e41a4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Habilite como code, y dessabilite la instanciacion de los otros modelos, dejandolo en modo raw\n",
    "\n",
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"mistral:latest\")\n",
    "#chat = ChatOllama(model=\"gemma3:4b\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df098fb-63ad-4865-8246-71d7120d545c",
   "metadata": {},
   "source": [
    "### Conexion mediante modelos de Groq"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de98efb9-a533-4e6c-9bbf-a0d9a098b8d1",
   "metadata": {},
   "source": [
    "# Habilite como code, y dessabilite la instanciacion de los otros modelos, dejandolo en modo raw\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6224761b-5c39-4f77-ba2b-8ed2a47b29eb",
   "metadata": {},
   "source": [
    "## Realizar peticiones mediante SystemMessage y HummanMessaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36bc66f-e88e-4846-9173-931930a0f85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se indica el prompt con el HumanMessage\n",
    "resultado = chat.invoke([HumanMessage(content= \"Indicame donde queda Pasto\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a9b13-eec9-43e7-9c8b-18965772b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos la respuesta, más otros datos\n",
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4c6fe8-16bc-4173-aa6e-f0cf14f6c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos solo con la respuesta limpia\n",
    "resultado.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0550e9d-ab0a-42b6-b1ea-c41effcdec07",
   "metadata": {},
   "source": [
    "## Especificando el SystemMessage\n",
    "\n",
    "Se especifica el systemMessage para definir la personalidad que debe tener el sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c4c49-d212-48ff-9247-d470ebbf167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado =chat.invoke([SystemMessage(content=\"Eres un historiador y experto en las ciudades Latinoamericanas\"), HumanMessage(\"Indicame donde queda San Gil\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8aed2-0a90-4998-94bb-952ac7150a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef2c61e-5440-4dfd-a72c-1f7e32cc90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe29f576-253f-422c-8a9c-2374d1056a76",
   "metadata": {},
   "source": [
    "## Obteniendo varios resultados invocando al chat de OpenAI con \"generate\"\n",
    "\n",
    "Mediante el metodo 'generate', se pueden hacer llamadas por lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e10fb-8909-4bdd-91df-25414c5a6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.generate(\n",
    "    [\n",
    "        [SystemMessage(content = 'Eres un historiador y experto en las ciudades Latinoamericanas'),\n",
    "         HumanMessage(content = 'Indicame donde queda Ocaña')],\n",
    "        [SystemMessage(content = 'Eres un joven rudo a quien no le gusta que le anden preguntando cosas, su único interés es salir de fiesta'),\n",
    "         HumanMessage(content = 'Indicame donde queda Cúcuta\"')]\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28fb4f-fe88-4f12-9f81-4491b33b2ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultado con el primer prompt sistema\n",
    "print(resultado.generations[0][0].text)\n",
    "\n",
    "# Resultado con el segundo prompt del sistema\n",
    "print(\"\\n\",resultado.generations[1][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d871f0b-78c6-413f-b6ca-d89f022aeca3",
   "metadata": {},
   "source": [
    "# Plantillas de Prompt en LangChain\n",
    "\n",
    "En LangChain, las **plantillas de Prompt** son estructuras predefinidas que facilitan la creación de mensajes personalizados y reutilizables para interactuar con modelos de lenguaje. Estas plantillas son fundamentales para estructurar las interacciones, integrar variables dinámicas y garantizar consistencia en los mensajes enviados. nos permite enviar las solicitudes(mensajes) como parámetros para estandarizar el proceso y facilitar la interacción con otros componentes de mis aplicaciones.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Qué son las plantillas de Prompt y para qué sirven?**\n",
    "\n",
    "Una plantilla de Prompt permite definir mensajes que pueden incluir contenido estático (texto fijo) y dinámico (variables que cambian según el contexto). Estas plantillas son útiles para estandarizar las entradas al modelo, reducir la repetición manual y generar mensajes adaptables.\n",
    "\n",
    "### **Beneficios:**\n",
    "- **Automatización:** Evitan escribir mensajes repetitivos.\n",
    "- **Flexibilidad:** Incorporan variables dinámicas para personalizar la interacción.\n",
    "- **Estandarización:** Aseguran un formato consistente en los mensajes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Clases principales de plantillas en LangChain**\n",
    "\n",
    "### **1. PromptTemplate**\n",
    "La clase básica para construir mensajes personalizados. Integra variables dinámicas y permite estructurar mensajes con contenido estático.\n",
    "\n",
    "#### **Propósito:**\n",
    "Generar mensajes base reutilizables que combinen texto fijo y valores dinámicos.\n",
    "\n",
    "#### **Ejemplo de uso:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f4e87-094f-4580-bdb7-3ea78ed22610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"nombre\"],\n",
    "    template=\"Hola, {nombre}. ¿Cómo puedo ayudarte hoy?\"\n",
    ")\n",
    "\n",
    "mensaje = prompt.format(nombre=\"Juan\")\n",
    "print(mensaje)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fa844-e62b-4ced-9306-7728cca17110",
   "metadata": {},
   "source": [
    "### **2. ChatPromptTemplate**\r\n",
    "\r\n",
    "`ChatPromptTemplate` es una plantilla diseñada para estructurar y organizar interacciones en el contexto de conversaciones con modelos de lenguaje. Permite combinar diferentes tipos de mensajes (como los mensajes del sistema, humanos y generados por IA) en un solo flujo coherente. \r\n",
    "\r\n",
    "Esta plantilla es útil cuando se necesita definir un diálogo que involucra múltiples etapas o participantes, ya que proporciona una estructura flexible para gestionar estas interacciones de manera ordenada. Además, facilita la personalización y reutilización de flujos conversacionales, adaptándose a las necesidades específicas de la aplicación.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **3. SystemMessagePromptTemplate**\r\n",
    "\r\n",
    "`SystemMessagePromptTemplate` se utiliza para generar mensajes del sistema que establecen el contexto general o las instrucciones iniciales para el modelo. Estos mensajes son clave para definir el comportamiento esperado del modelo durante la interacción y garantizar que las respuestas estén alineadas con el propósito de la aplicación.\r\n",
    "\r\n",
    "Es especialmente útil para configurar el tono, las reglas o el enfoque que debe seguir el modelo en el manejo de las consultas, proporcionando un marco claro para la conversación.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **4. HumanMessagePromptTemplate**\r\n",
    "\r\n",
    "`HumanMessagePromptTemplate` es una plantilla destinada a representar las entradas o consultas realizadas por un usuario humano en el contexto de la interacción con el modelo. Su objetivo es estandarizar y estructurar las preguntas o comentarios humanos para asegurar consistencia y claridad en el flujo de mensajes.\r\n",
    "\r\n",
    "Esta plantilla resulta adecuada para personalizar mensajes según las necesidades de la interacción, permitiendo incluir elementos dinámicos o específicos proporcionados por los usuarios.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **5. AIMessagePromptTemplate**\r\n",
    "\r\n",
    "`AIMessagePromptTemplate` está diseñada para construir mensajes que simulan las respuestas generadas por la inteligencia artificial dentro de una conversación. Estos mensajes son útiles para predefinir o estandarizar cómo deben ser las respuestas de la IA en escenarios controlados o planificados.\r\n",
    "\r\n",
    "Esta plantilla permite que los desarrolladores ajusten la manera en que la IA responde en el contexto de flujos conversacionales, asegurando que los mensajes sean coherentes y alineados con el propósito del sistema.\r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759b39d6-6a35-49e9-a03d-d2de432e4496",
   "metadata": {},
   "source": [
    "| **Plantilla**               | **Propósito**                                                                 | **Uso principal**                                                                                       |\r\n",
    "|-----------------------------|------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|\r\n",
    "| **PromptTemplate**           | Genera mensajes básicos combinando texto estático y variables dinámicas.      | Crear prompts reutilizables y personalizables con contenido dinámico.                                 |\r\n",
    "| **ChatPromptTemplate**       | Estructura flujos conversacionales combinando mensajes de distintos tipos.    | Crear interacciones complejas y organizadas entre humanos, sistema e inteligencia artificial.         |\r\n",
    "| **SystemMessagePromptTemplate** | Define mensajes del sistema para establecer contexto y reglas iniciales.       | Configurar el comportamiento esperado del modelo durante la interacción.                              |\r\n",
    "| **HumanMessagePromptTemplate**  | Representa las consultas o entradas realizadas por un usuario humano.         | Estandarizar y personalizar preguntas o comentarios del usuario.                                      |\r\n",
    "| **AIMessagePromptTemplate**     | Construye mensajes que simulan respuestas generadas por la inteligencia artificial. | Predefinir y controlar las respuestas de la IA en flujos conversacionales planificados.               |\r\n",
    "             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57356d0f-20fd-4d8f-bffa-e887c94b3f44",
   "metadata": {},
   "source": [
    "### Importar librerias para templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1ebef-6384-4dc9-a232-5c7a2e88cfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706f5c6-1e78-4cbf-a1ad-677adf0eb8af",
   "metadata": {},
   "source": [
    "### Generar plantillas de prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70362e-641f-49f4-8c1c-0b6d4c13234e",
   "metadata": {},
   "source": [
    "#### 1. Crear plantillas del sistema (system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9186dc-4284-4e7b-aa7c-2894e7f715bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Eres una IA especializada en automóviles de tipo {tipo_automovil} y en generar artículos que se leen en {tiempo_lectura}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e38284-7ce0-41df-8f9f-9f7231b942e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c7444-6a6e-4686-a0ed-b9e9445ddcbf",
   "metadata": {},
   "source": [
    "#### 2. Crear la plantilla del usuario (human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51791452-27b6-41a8-8bb2-ec5e89b3a749",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = \"Necesito un artículo para vehículos con motor {peticion_tipo_motor}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb8c253-1f51-40fd-b96e-83b8e838a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8ecd7-6089-44a7-8726-ad965264672e",
   "metadata": {},
   "source": [
    "#### 3. Creamos una plantilla de chat con la concatenación tanto de mensajes del sistema como del humano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ef74c1-e465-49ef-8f6c-6ed012fa3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c0ffca-7cf8-450d-ad1d-c7ea96cb6f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04222d-ab78-430e-8588-6a7e31282f84",
   "metadata": {},
   "source": [
    "#### 4. Completar el chat gracias al formateo de los mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926f8b76-d584-4251-978c-2028acb8dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt.format_prompt(peticion_tipo_motor = \"híbrido enchufable\",tiempo_lectura = \"5 minutos\", tipo_automovil = \"japonés\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d99b0b9-c97b-40af-ae10-51dc0afb2b73",
   "metadata": {},
   "source": [
    "#### 5. Se transforma el objeto prompt a una lista de mensajes y se guarda en una variable para enviar al LLM\n",
    "EL objeto prompt se guarda en la variables **solicitud_completa** que es la que se enviará finalmente al LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d350598-004b-41c2-9d11-f6cbdba83232",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa = chat_prompt.format_prompt(peticion_tipo_motor = \"hibrido\",tiempo_lectura = \"3 minutos\", tipo_automovil = \"Europea\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d055bc9-91c3-40aa-ba15-5b4954ca0a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d5b04-5d99-4bfe-8d33-a26f5e8fabf0",
   "metadata": {},
   "source": [
    "### Obtener el resultado de la respuesta formateada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21478396-5b77-448d-b1df-b6e4a39fe91f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d935f4e8-8401-4b22-87e9-a2315a50f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexion con OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env (busca automáticamente en el directorio actual o superiores)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró OPENAI_API_KEY en el .env ni en el entorno\")\n",
    "\n",
    "# Crear instancia del modelo\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),   # sk-proj-...\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d734255-4c82-4ea3-9573-4f5dbf66757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat.invoke(solicitud_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ffb46-d8ce-47b6-8aeb-e381b4736781",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96e9cda-d53d-4b48-bcd0-d0969ee770b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51982526-99cb-4f96-9c1d-f3e4e6c4c6fa",
   "metadata": {},
   "source": [
    "## Aplicamos el template a un modelo de Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54122b5e-7fb1-4c53-a910-b46c11654df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chatGroq = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190f8bc-14cd-49b5-a3d3-ec30a8b737c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa = chat_prompt.format_prompt(peticion_tipo_motor = \"eléctrico\",tiempo_lectura = \"2 minutos\", tipo_automovil = \"Americano\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387419e-862e-4bc2-b300-f58695bb9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadoGroq = chatGroq.invoke(solicitud_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5bfc13-0296-4f0c-9991-4bf47ccc6821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultadoGroq.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68b1afa-1921-42aa-8ba4-070ee5838038",
   "metadata": {},
   "source": [
    "# Procesamiento de datos de Salida Mediante LangChain - Parseo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502283f-5cfe-4eb7-bfff-e4bf6885e7f5",
   "metadata": {},
   "source": [
    "En LangChain, el **parseo** es el proceso de interpretar y transformar datos de un formato a otro, asegurando que la información sea comprensible y utilizable dentro de un flujo o aplicación. Este concepto es fundamental cuando se trabaja con modelos de lenguaje, ya que los datos de entrada y salida necesitan ser procesados para garantizar coherencia y usabilidad.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Para qué sirve el parseo en LangChain?**\n",
    "\n",
    "El parseo permite estructurar y manipular los datos generados por los modelos de lenguaje o ingresados por los usuarios. Sirve para:\n",
    "- **Interpretar respuestas:** Extraer información específica de las respuestas del modelo en formatos estructurados (como JSON, tablas, o listas).\n",
    "- **Validar entradas:** Garantizar que los datos ingresados cumplan con el formato esperado antes de ser procesados.\n",
    "- **Preparar datos:** Convertir información compleja en un formato sencillo que el modelo o aplicación pueda entender fácilmente.\n",
    "\n",
    "---\n",
    "\n",
    "## **¿Qué tipo de cosas se pueden hacer mediante el parseo?**\n",
    "\n",
    "LangChain ofrece herramientas para realizar diversas tareas relacionadas con el parseo, entre ellas:\n",
    "- **Extraer información clave:** Identificar y extraer entidades, números, fechas, o cualquier dato específico en una respuesta.\n",
    "- **Transformar formatos:** Convertir texto plano a estructuras como JSON, XML, listas o tablas.\n",
    "- **Dividir texto:** Separar grandes bloques de texto en segmentos más pequeños para su análisis o procesamiento.\n",
    "- **Normalizar datos:** Estandarizar respuestas o entradas para garantizar consistencia en el flujo de trabajo.\n",
    "- **Integrar lógica de negocio:** Interpretar respuestas del modelo y adaptarlas a las necesidades específicas de una aplicación.\n",
    "\n",
    "---\n",
    "\n",
    "## **Aplicaciones del parseo en LangChain**\n",
    "\n",
    "El parseo tiene múltiples aplicaciones prácticas en sistemas impulsados por modelos de lenguaje:\n",
    "1. **Sistemas de preguntas y respuestas:**\n",
    "   - Extraer respuestas específicas de documentos largos.\n",
    "   - Convertir respuestas en listas de puntos clave para una mejor comprensión.\n",
    "\n",
    "2. **Chatbots y asistentes virtuales:**\n",
    "   - Interpretar comandos del usuario y traducirlos en acciones específicas.\n",
    "   - Validar entradas para asegurarse de que estén en un formato esperado (por ejemplo, fechas o ubicaciones).\n",
    "\n",
    "3. **Análisis de texto:**\n",
    "   - Identificar entidades clave como nombres, lugares o valores numéricos.\n",
    "   - Procesar datos generados para alimentar otras herramientas o sistemas.\n",
    "\n",
    "4. **Integración con otros sistemas:**\n",
    "   - Convertir respuestas en formatos compatibles con APIs externas, como JSON o XML.\n",
    "   - Traducir salidas del modelo en datos estructurados para bases de datos o visualización.\n",
    "\n",
    "5. **Automatización de tareas complejas:**\n",
    "   - Procesar y organizar grandes cantidades de texto.\n",
    "   - Estandarizar formatos paita la creación de aplicaciones robustas y precisas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfc318-c38b-4308-98f4-1a75ae97754f",
   "metadata": {},
   "source": [
    "## Ejemplo 1. Parsear como lista separada por comas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2c631-3d7c-49c1-b156-88ed311aafc7",
   "metadata": {},
   "source": [
    "- Parar probar este ejemplo no necesitamos generar la respuesta con el LLM. Supongamos que tenemos una entrada de algunos elementos y necesitamos que la convierta a una entrada lista separada por comas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bfa49-de24-400d-a79c-301fc6f16fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una respuesta imaginaria o supuesta obtenida del LLM\n",
    "\n",
    "respuestaLLM = \"Fútbol, Baloncesto, Ciclismo, Voleibol\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581f6c40-a726-4875-94ff-edc680ede94b",
   "metadata": {},
   "source": [
    "- Utilizaremos el **parseador** `CommaSeparatedListOutputParser` para dar fomato requerido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44ea6b-38a3-45fa-9dcd-bb0e66dc5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions() # que devuelve las instrucciones que va a pasar al LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e990b-a058-4b2d-8c66-b7d5728ce7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(format_instructions)  # podemos verificar las instrucciones contenidas en el parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e2c49-8dc8-4153-8bc2-4fed66b484a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hacemos el llamado al parser y damos formato a la entrada\n",
    "respuestaFormateada = output_parser.parse(respuestaLLM)\n",
    "respuestaFormateada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394165aa-5aaa-4324-9930-2f69a8770d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifiquemos el tipo de la salida\n",
    "type(respuestaFormateada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61139f7b-2217-4d9b-aa79-843a5e23fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¿Cual era el formato de entrada?\n",
    "type(respuestaLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f8e99-6dd4-471b-9b83-1e2ce3bc6d68",
   "metadata": {},
   "source": [
    "### Parsear una respuesta obtenida el LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca1e84-6c05-47b3-984d-6bee0f97ce05",
   "metadata": {},
   "source": [
    "- En este caso vamos a crear una plantilla de usuario (HumanTemplate) con la concatenación de la variable 'request' (la solicitud) y la variable 'format_instructions', con las instrucciones adicionales que pasaremos al LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da085d17-3755-4742-8f22-96a927e5bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_template = '{request}\\n{format_instructions}'\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743febfd-e65d-4c23-8624-dfdd4e3592be",
   "metadata": {},
   "source": [
    "- Ahora creamos el prompt sobre la plantilla e instanciamos las variables, asignándoles la cadena de texto con la solicitud y las indicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569fde9-2ab8-4518-8d09-287757da717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "chat_prompt.format_prompt(request = \"dime 5 características de los automóviles chinos\",\n",
    "                          # Se proporcionan las instrucciones obtenidas del parseador\n",
    "                          format_instructions = output_parser.get_format_instructions())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327a175-17d4-4f71-b7a9-ec7a22a540f8",
   "metadata": {},
   "source": [
    "- Ahora se obtiene a ártir del objeto prompt el texto de la petición y se guarda en 'solicitud_completa' que es la que finalmente se pasará al LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1696574-35b7-469d-8ef6-c208beb04dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "solicitud_completa = chat_prompt.format_prompt(request = \"dime 5 características de los automóviles coreanos\",\n",
    "                          format_instructions = \"responde unicamente con las 5 características separados por comas\")\n",
    "\n",
    "print(solicitud_completa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba666f-1172-4af5-8ddb-0b1cf49fe1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.invoke(solicitud_completa)\n",
    "resultado.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd8c93-62b5-4fe4-b041-aa2237f6649a",
   "metadata": {},
   "source": [
    "- Para que el formato de salida sea preciso (lista separada por comas) aplicamos el parseador de LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625488c-9660-4270-9e9e-97d9874cfa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = output_parser.parse(resultado.content)\n",
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a58167-64df-4f7b-989d-050fa9e1ad5a",
   "metadata": {},
   "source": [
    "# Probando diferentes entradas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd471a92-1dca-41f3-8137-1855cc5f8428",
   "metadata": {},
   "source": [
    "- Probemos como se compoarta el parseador antes diferentes hipotéticas salidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44424c7e-938d-4506-a381-40588ed2e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instancia del parser\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# Lista de entradas de prueba\n",
    "entradas = [\n",
    "    \"Manzana, Banana, Pera, Mango\",\n",
    "    \" Sol ,Luna ,  Estrellas \",\n",
    "    \"Auto,,Bicicleta\",\n",
    "    \"Agua\",\n",
    "    \"\",\n",
    "    \"Azul, rojo, , verde,  , amarillo\",\n",
    "    \"\\\"uno\\\", \\\"dos\\\", \\\"tres\\\"\",\n",
    "    \"123, 456 ,789 \",\n",
    "    \"   , ,    ,dato   ,  ,final\",\n",
    "    \"A, B,C , D ,E\"\n",
    "]\n",
    "\n",
    "# Procesar y mostrar resultados\n",
    "for i, entrada in enumerate(entradas, start=1):\n",
    "    respuesta = output_parser.parse(entrada)\n",
    "    print(f\"Entrada {i}: {entrada}\")\n",
    "    print(f\"Salida  {i}: {respuesta}\")\n",
    "    print(f\"Tipo    {i}: {type(respuesta)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8290d9-1f37-4235-9669-a31a6970e403",
   "metadata": {},
   "source": [
    "## Ejemplo 2. Parsear datos de fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85dc2a0d-3abc-4e29-9627-26a4689b32e7",
   "metadata": {},
   "source": [
    "- Es común obtener datos de fechas solicitados al LLM, y en muchos casos es útil convertirlo a un objeto `datatime`.\n",
    "- `datetime` es una clase del módulo estándar de Python que representa un punto específico en el tiempo, con la posibilidad de incluir:\n",
    "    - Año, mes, día. \n",
    "    - Hora, minuto, segundo, microsegundo. \n",
    "    - Zona horaria (opcional).\n",
    "\n",
    "- `DatetimeOutputParser` es una clase especializada para interpretar la respuesta de un modelo de lenguaje y convertirla en un objeto datetime de Python.\n",
    "- Utiliza por defecto el formato \"%Y-%m-%dT%H:%M:%S.%fZ\", es decir, el formato ISO-8601 con zona UTC. Esto se puede personalizar según tus necesidades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8193afa1-3884-4c7c-934a-a30b054f5f2e",
   "metadata": {},
   "source": [
    "- **En el siguiente bloque importamos, instanciamos y verificamos las instrucciones del parseador:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae21db9-4cab-4aa1-a8d6-8afdef6e1a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos la clase\n",
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "\n",
    "# Instaciamos el parser\n",
    "output_parser = DatetimeOutputParser()\n",
    "\n",
    "# Vemos las instrucciones que se dan al LLM\n",
    "print(output_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727acba-8bdf-471d-a3a3-7c8549b3c02a",
   "metadata": {},
   "source": [
    "- **Ahora crearemos las plantillas para hacer solicitud y dar las indicaciones como en el caso anterior:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64fbfd-4285-47a3-ba04-256eec503db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea la plantilla\n",
    "template_text = \"{request}\\n{format_instructions}\" \n",
    "\n",
    "# Se crea la plantilla para el prompt del usuario\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(template_text) \n",
    "\n",
    "# Se crea la plantilla de chat lista para enviar al LLM, a partir de la plantilla del usuario\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt]) \n",
    "\n",
    "# Podemos ver la forma que toma la solicitud completa\n",
    "print(chat_prompt.format_prompt(request=\"Cuándo es el dia de la declaración de independencia de Colombia\",\n",
    "                         format_instructions = output_parser.get_format_instructions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac66a1b-01fc-4da4-97b4-b27118088bd0",
   "metadata": {},
   "source": [
    "- **Creamos la solicitud completa la enviamos al LLM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96889df9-7cbf-433d-9547-b3f430769258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solicitud completa\n",
    "solicitud_completa = chat_prompt.format_prompt(request=\"Cuándo es el dia de la declaración de independencia de Colombia\",\n",
    "                         format_instructions = output_parser.get_format_instructions()).to_messages()\n",
    "# envío al LLM\n",
    "respuesta = chat.invoke(solicitud_completa)\n",
    "#Verificamos la salida\n",
    "respuesta.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd6b5f8-994f-4bba-8de7-8ff076a6bf3f",
   "metadata": {},
   "source": [
    "- **Ahora llamamos al parser que me devuelve el objeto `datetime` a partir de lo obtenido del LLM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0ec054-2078-4ed2-8104-1201e3be134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fechaObtenida = output_parser.parse(respuesta.content)\n",
    "fechaObtenida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3690bca9-cd92-491c-ac2d-aac70524f001",
   "metadata": {},
   "source": [
    "- **Con el objeto obtenido puedo calcular tiempo transcurrido hasta hoy, facilmente:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512a4e9-aaa7-4855-8d43-8e2ea9e9edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# Instalar primero si no la tienes:\n",
    "# pip install python-dateutil\n",
    "\n",
    "fechaActual = datetime.now()\n",
    "\n",
    "diferencia = relativedelta(fechaActual, fechaObtenida)\n",
    "\n",
    "print(f\"Han pasado {diferencia.years} años, {diferencia.months} meses y {diferencia.days} días.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49eeec8-6dae-41ad-9a8d-de68963eb8bc",
   "metadata": {},
   "source": [
    "## Solución de problemas de Parseo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de0d23b-37f9-4374-8ff9-350783e1d09c",
   "metadata": {},
   "source": [
    "### Auto-Fix Parser en LangChain // Analizador sintáctico de fijación automática\n",
    "\n",
    "El **Auto-Fix Parser** es una utilidad de LangChain diseñada para manejar y corregir automáticamente errores en las respuestas generadas por modelos de lenguaje. Es especialmente útil cuando el modelo debe generar salidas en un formato específico (por ejemplo, JSON) y estas contienen errores o inconsistencias.---\n",
    "\n",
    "#### **¿Qué es el Auto-Fix Parser?**\n",
    "Es una herramienta que:\n",
    "1. **Valida salidas**: Detecta errores en la salida generada por el modelo.\n",
    "2. **Corrige automáticamente**: Solicita al modelo correcciones iterativas hasta obtener un formato válido o alcanzar un límite predefinido.\n",
    "3. **Automatiza flujos**: Evita intervenciones manuales en la validación de respuestas.\n",
    "\n",
    "---\n",
    "\n",
    "### **Casos de Uso**\n",
    "- **Generación de formatos estrictos**: JSON, listas, estructuras numéricas.\n",
    "- **Integración con sistemas externos**: Cuando las respuestas deben cumplir requisitos específicos.\n",
    "- **Entornos de producción**: Aumenta la confiabilidad de pipelines al manejar errores automáticamente.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Características principales**\n",
    "1. **Validación y corrección automática**:\n",
    "   - Solicita al modelo corregir errores detectados.\n",
    "2. **Configuración personalizable**:\n",
    "   - Define formatos esperados o reglas de validación.\n",
    "3. **Iteraciones limitadas**:\n",
    "   - Establece un número máximo de intentos para evitar ciclos infinitos.\n",
    "4. **Integración con LangChain Chains**:\n",
    "   - Funciona dentro de flujos de trabajo estructurados.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0415788-ef47-433f-bede-2c5aed9e54a0",
   "metadata": {},
   "source": [
    "### Ejemplo practico: Error en la generación de fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6301edc-22f5-42a6-95e1-38c2d16bbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciamos el parser que queremos\n",
    "output_parser_dates = DatetimeOutputParser() \n",
    "\n",
    "# obtenemos la respuesta, hipotéticamente incorrecta\n",
    "misformatted = respuesta.content \n",
    "misformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b37749a-b0d9-4c72-8fd2-aa7b27fe3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiemos la salida por una que sería incorrecta\n",
    "misformatted = '1810|07-20T00:00:00.000000Z'\n",
    "misformatted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca920654-d61a-4b3c-ab6b-64106b4605f1",
   "metadata": {},
   "source": [
    "- **¿Qué ocurre si aplicamos aqui el parseador?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c392e-9250-479d-98b8-58ad0ab08efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fechaObtenida = output_parser.parse(misformatted)\n",
    "#fechaObtenida\n",
    "\n",
    "#  Capturemos el error al parsear\n",
    "\n",
    "# Debemos importar la clase que atrape la excepción\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "try:\n",
    "    fechaObtenida = output_parser.parse(misformatted)\n",
    "    print(\"Fecha parseada:\", fechaObtenida)\n",
    "except OutputParserException as e:\n",
    "    print(\"Error de parseo:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d27082-4d23-41af-a928-0717d31fa7ad",
   "metadata": {},
   "source": [
    "- **Entonces, que pasa si aplicamos el autofix-parser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1580a-164c-41b6-8e78-d4b72c86bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clase para parsear con Auto-Fix, corregir iterativamente hasta obtener la salida esperada\n",
    "from langchain.output_parsers import OutputFixingParser \n",
    "\n",
    "# creamos otro parser iterativo a partir de la fecha y dirigido a mi LLM\n",
    "new_parser = OutputFixingParser.from_llm(parser = output_parser_dates,llm=chat)\n",
    "\n",
    "# Se aplica el parseo y vemos la respuesta\n",
    "new_parser.parse(misformatted) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5602d218-d0ba-474b-9c8f-606a94f0148d",
   "metadata": {},
   "source": [
    "### Refuerzo del parseo mediante prompt del sistema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eec82e-b040-46ed-a757-5b4fa96d4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"Tienes que responder únicamente con un patrón de fechas\")\n",
    "\n",
    "human_template = \"{request}\\n{format_instructions}\"  # Para el template del usuario\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_prompt,human_prompt])\n",
    "\n",
    "solicitud_completa = chat_prompt.format(request = \"¿Cuándo es el dia de la declaración de la independencia de USA?\",\n",
    "                         format_instructions = output_parser_dates.get_format_instructions() )\n",
    "\n",
    "print(solicitud_completa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b3b98-4b8c-4ec3-a82b-ddeb4d7e28da",
   "metadata": {},
   "source": [
    "- **Luego se envía la solicitud con el al LLM y esta su respuesta se pasa al parseador**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0dbbc-c090-4001-a8f7-c13363546d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.invoke(solicitud_completa)\n",
    "print(resultado.content)\n",
    "output_parser_dates.parse(resultado.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c95a57-0b6d-4502-99e4-ab83ddf55789",
   "metadata": {},
   "source": [
    "### Una ejemplo adicional: Salida en formato json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765db4f0-597d-4003-83c1-43b459e80afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "# Crear los mensajes directamente\n",
    "messages = [\n",
    "    SystemMessage(content=\"\"\"\n",
    "Eres un asistente que responde exclusivamente en formato JSON válido.\n",
    "No debes incluir texto explicativo, encabezados, ni ningún contenido fuera del JSON.\n",
    "Todos los objetos deben estar bien formateados para ser interpretados por parsers automáticos.\n",
    "\"\"\"),\n",
    "    HumanMessage(content=\"\"\"\n",
    "Dame la lista de los 11 jugadores titulares que iniciaron con España en la final del Mundial de Fútbol 2010.\n",
    "\n",
    "Devuélveme solo el resultado en formato JSON.  \n",
    "Cada jugador debe representarse como un objeto con las siguientes claves:\n",
    "- \"dorsal\"\n",
    "- \"apellido\"\n",
    "- \"nombre\"\n",
    "- \"posición\"\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "# Inicializar el modelo\n",
    "\n",
    "\n",
    "# Llamar directamente con los mensajes\n",
    "respuesta_json = chat.invoke(messages)\n",
    "\n",
    "# Imprimir el contenido generado\n",
    "print(respuesta_json.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e3c36-6d45-4768-85db-dc739c875f65",
   "metadata": {},
   "source": [
    "- **Podemos pasar la salida a un parseador de json a dict y cargarlo en un dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f047b2-d0b3-421d-b56a-1e5e09b56126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.json import JsonOutputParser\n",
    "\n",
    "# Crear el parser\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "# Parsear la respuesta recuperada\n",
    "parsed_data = parser.parse(respuesta_json.content)\n",
    "\n",
    "# Ver resultado\n",
    "print(parsed_data)\n",
    "print(type(parsed_data))  # List[Dict]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231af406-ab8b-4ddc-b7c1-dd336f846096",
   "metadata": {},
   "source": [
    "- **Esta información puede ser cargada directamente a un dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212708e7-1a00-42f6-8fb1-7c76e59f9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Seleccionamos la info en a lista de diccionario\n",
    "jugadores = parsed_data\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(jugadores)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea647a2-0ecf-4ce1-81d2-5a84831d1057",
   "metadata": {},
   "source": [
    "## Reutilización de prompts mediante serialización"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f8ca5-1663-43cf-bbe1-04f38cdf198b",
   "metadata": {},
   "source": [
    "- Es posible que desees guardar, compartir o cargar objetos de prompts. \n",
    "- Langchain permite guardar fácilmente plantillas de mensajes como archivos JSON para leer o compartir\n",
    "- Muy útil cuando hay plantillas complejas que distribuir o reutilizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81662ce5-3c1a-44a7-89dd-b97262d29c4b",
   "metadata": {},
   "source": [
    "#### Guardar la plantilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42de943-646c-40c3-9268-a133db5823ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantilla = \"Pregunta: {pregunta_usuario}\\n\\nRespuesta: Vamos a verlo paso a paso.\"\n",
    "prompt = PromptTemplate(template=plantilla)\n",
    "prompt.save(\"prompt.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0650ac7-1242-4e40-a800-da6600edf462",
   "metadata": {},
   "source": [
    "#### Cargar la plantilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568019b-d744-4ee5-b74d-b065ebb441ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la clase load_prompt\n",
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt_cargado = load_prompt('prompt.json')\n",
    "\n",
    "# Verficiando el contenido cargado\n",
    "prompt_cargado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be926d2c-d10b-4713-be58-35cfd41a6c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f73d59-4b38-4709-86b0-1bce065011fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
