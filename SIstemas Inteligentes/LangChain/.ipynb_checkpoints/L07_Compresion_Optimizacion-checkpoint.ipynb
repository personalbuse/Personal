{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02362fb4-e9ce-44c6-a990-742f8fbd2360",
   "metadata": {},
   "source": [
    "# Compresión y optimización de datos obtenidos en BD mediante LLM's\n",
    "\n",
    "\r\n",
    "En un flujo típico de trabajo con modelos de lenguaje y documentos largos, se pasa por varias etapas antes de obtener una respuesta útil y precisa. Uno de los pasos más importantes es **optimizar o comprimir la información recuperada** para que el modelo pueda utilizarla de manera efectiva.\r\n",
    "\r\n",
    "### Flujo general de trabajo\r\n",
    "\r\n",
    "1. **Carga del documento**\r\n",
    "   - Se utiliza un loader para cargar archivos desde diversas fuentes de datos (PDF, CSV, Google Drive, etc.).\r\n",
    "\r\n",
    "2. **División en fragmentos (chunks)**\r\n",
    "   - Los documentos se dividen en fragmentos de texto manejables mediante transformadores como `CharacterTextSplitter`.\r\n",
    "\r\n",
    "3. **Generación de vectores**\r\n",
    "   - Cada fragmento se convierte en un vector numérico utilizando una función de *embedding*. Estos vectores representan el significad\n",
    "\n",
    "\n",
    "4. **Almacenamiento en una base de datos vectorial**\r\n",
    "- Los vectores generados se almacenan en una Vector DB (como FAISS, Chroma o SKLearnVectorStore) para realizar búsquedas por similitud.\r\n",
    "\r\n",
    "5. **Búsqueda por similitud**\r\n",
    "- Ante una consulta del usuario, se genera un vector y se comparan distancias con los vectores almacenados.\r\n",
    "- Se devuelve un *ranking* con los fragmentos más similares.\r\n",
    "\r\n",
    "6. **Compresión u optimización con un LLM**\r\n",
    "- En lugar de pasar toda la información recuperada al modelo, se puede utilizar el LLM para generar una **respuesta más corta y rel\n",
    "  evante**.\r\n",
    "- Este paso no es una compresión en el sentido técnico de reducir tamaño de archivo, sino en el sentido semántico: el LLM sintetiza el contenido para dar una **respuesta más enfocada, útil y clara**.\r\n",
    "\r\n",
    "### ¿Por qué es importante esta optimización?\r\n",
    "\r\n",
    "- **Limita la cantidad de tokens que se envían al modelo**, lo cual puede afectar el costo y rendimiento.\r\n",
    "- **Mejora la precisión y relevancia de la respuesta final**, al evitar que el modelo se \"distraiga\" con información irrelevante.\r\n",
    "- **Permite construir asistentes más eficientes**, como chatbots que respondan basándose solo en los fragmentos más significativos del documento original.\r\n",
    "\r\n",
    "Este enfoque es especialmente útil para aplicaciones de recuperación aumentada por generación (*Retrieval-Augmented Generation* o RAG), donde se utiliza información externa para mejorar las respuestas de los modelos de lenguaje.\r\n",
    "oo de vectores:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bba99-c0f1-4fb1-94f5-87173a1f53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las lirerías\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8d65-e517-4c0d-9c79-3c88f17155c2",
   "metadata": {},
   "source": [
    "## Carga documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc698049-886f-481e-b89c-033a1d57e2f0",
   "metadata": {},
   "source": [
    "- Vamos a realizar una consulta puntual sobre Python, para lo cual, iniciamos cargando la información disponible en wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c031a92-1b0a-4789-9a2e-de5d05207bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Orlando\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file C:\\Users\\Orlando\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "loader = WikipediaLoader(query = 'Lenguaje Python', lang = 'es')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3310948-09dc-4b73-afcb-86df28802f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f4431e-ecfb-40d3-8882-92919ef9cf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b83c1-f157-4fd8-b70f-066fb3aa24eb",
   "metadata": {},
   "source": [
    "## Split de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38f7fa30-3efc-42f0-abe7-ad6e020853cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04756f75-22f9-49fa-8a40-c241e095b670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 629, which is longer than the specified 500\n",
      "Created a chunk of size 515, which is longer than the specified 500\n",
      "Created a chunk of size 591, which is longer than the specified 500\n",
      "Created a chunk of size 542, which is longer than the specified 500\n",
      "Created a chunk of size 653, which is longer than the specified 500\n",
      "Created a chunk of size 714, which is longer than the specified 500\n"
     ]
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4937165a-9355-46f4-96be-86df3f55eb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50948368-187e-4acb-bb34-96e22b08eb66",
   "metadata": {},
   "source": [
    "## Cargar el modelo embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4361bbe-591f-40e7-a1a3-0e62dfe60838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Detectar GPU si está disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instanciar LaBSE\n",
    "funcion_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/LaBSE\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba646959-d831-4131-be2e-5f6aee56eb0c",
   "metadata": {},
   "source": [
    "## Incustar documentos en BD de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fbfdab-4c70-4d38-af65-af504cc72548",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"./ejemplo_wiki_bd\"  # ruta donde se guarda la BD vectorizada\n",
    "\n",
    "# Se crea la BD de vectores a partir de los documentos y la función de embeddings\n",
    "vector_store = SKLearnVectorStore.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = funcion_embedding,\n",
    "    persist_path = persist_path,\n",
    "    serializer = \"parquet\"\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda74841-461a-4258-b64a-1fbbcf51713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c6739-18c0-4c0d-bef7-cacb942a987f",
   "metadata": {},
   "source": [
    "## Consulta normal similitud del coseno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b9401-4daa-470d-a99c-b6f21bb79ac1",
   "metadata": {},
   "source": [
    "- **Creamos un nuevo documento que será la consulta para buscar la mayor similitud en la BD usando la distancia del coseno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f254eb1-a522-4ac5-9fd4-9372a1acea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPy es un intérprete y compilador JIT para el lenguaje Python, que se enfoca en la velocidad y eficiencia, y es 100% compatible con el intérprete original CPython. \n",
      "\n",
      "\n",
      "== Detalles y motivación ==\n",
      "PyPy fue concebido como una implementación de Python escrita en Python, lo que permite a los desarrolladores Python hackear el lenguaje. Esto hace que sea fácil identificar áreas mejorables. El hecho de que PyPy esté implementado en un lenguaje de alto nivel implica que es más flexible y fácil para experimentar que CPython, lo que permite a los desarrolladores explorar múltiples implementaciones de características específicas y elegir la mejor. \n",
      "PyPy tiene por objeto proporcionar una traducción común y un framework conceptual para la producción de implementaciones de lenguajes dinámicos, haciendo hincapié en una separación limpia entre la especificación del lenguaje y los aspectos de implementación. Intenta además proporcionar una implementación compatible, flexible y rápida del Lenguaje Python utilizando el mencionado framework para desarrollar nuevas características avanzadas sin tener que codificar detalles a bajo nivel.[1]\n"
     ]
    }
   ],
   "source": [
    "consulta = \"¿Por qué el lenguaje Python se llama así?\"\n",
    "docs = vector_store.similarity_search(consulta)\n",
    "print(docs[3].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3a85e-adb6-436b-a633-ed664c142ec4",
   "metadata": {},
   "source": [
    "\n",
    "## Compresión Contextual de Documentos con LangChain y LLM´s\r\n",
    "\r\n",
    "Cuando realizamos una búsqueda en una base de datos vectorial, los resultados pueden incluir múltiples fragmentos de texto relevantes. Sin embargo, no siempre es eficiente pasar todo ese contenido al modelo. Para resolver esto, LangChain ofrece herramientas que permiten **comprimir y optimizar** esos fragmentos antes de enviarlos al LLM.\r\n",
    "\r\n",
    "Dos componentes clave para esto son:\r\n",
    "\r\n",
    "### `ContextualCompressionRetriever`\r\n",
    "\r\n",
    "Esta clase actúa como un *envoltorio* (wrapper) alrededor de otro retriever. Su objetivo es mejorar los resultados originales aplicando un proceso de compresión o filtrado adicional. Lo que hace es:\r\n",
    "\r\n",
    "- Recuperar documentos normalmente desde una base vectorial.\r\n",
    "- Pasar esos documentos a un *compresor* (por ejemplo, un LLM).\r\n",
    "- Devolver solo la versión comprimida o más relevante al modelo final.\r\n",
    "\r\n",
    "Se usa para reducir la cantidad de texto innecesario que se le envía al LLM, optimizando tanto el rendimiento como la calidad de las respuestas.\r\n",
    "\r\n",
    "### `LLMChainExtractor`\r\n",
    "\r\n",
    "Este es un tipo de *compresor de documentos* que utiliza un modelo de lenguaje (LLM) para extraer la parte más relevante de cada fragmento. En lugar de devolver el texto completo, devuelve una versión resumida, filtrada o ajustada del contenido.\r\n",
    "\r\n",
    "- Funciona como una cadena (`Chain`) de LangChain con un prompt predefinido.\r\n",
    "- Es útil para cuando se necesita pasar solo el *contexto esencial* al modelo, especialmente en aplicaciones tipo pregunta-respuesta (Q&A).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ¿Cómo se usan juntos?\r\n",
    "\r\n",
    "Ambas clases se combinan para construir un flujo donde se recuperan documentos y luego se comprimen antes de enviarlos al modelo:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a970447-a239-41cf-8997-8f63c3aa398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28db8b-f926-459a-854d-b221a10add87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5883da7a-0f6b-4e80-9b56-dd0b0a0b835a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola! Como soy una inteligencia artificial, no tengo sentimientos o emociones como los seres humanos, por lo que no estoy realmente \"bien\" o \"mal\". Sin embargo, estoy funcionando correctamente y lista para ayudarte en lo que necesites.\n",
      "\n",
      "En cuanto a quién soy, soy LLaMA, un modelo de lenguaje basado en inteligencia artificial desarrollado por Meta AI. Mi función es entender y responder a preguntas, proporcionar información, generar texto y mantener conversaciones en lenguaje natural. No tengo una identidad personal, pero estoy diseñada para ser amigable y útil. ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "# Creamos un nuevo documento que será la consulta para buscar la mayor similitud en la BD usando la distancia del coseno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Probar conexión\n",
    "respuesta = chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\")\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d182bb22-5093-4152-b202-1b6eede25aaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'compressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m compression_retriever = ContextualCompressionRetriever(base_compressor = compressor, base_retriever = vector_store.as_retriever())\n",
      "\u001b[31mNameError\u001b[39m: name 'compressor' is not defined"
     ]
    }
   ],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor = compressor, base_retriever = vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f3e7d21-4fff-4b15-867f-da2d3422d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(\"¿por qé el lenguaje Python se llama asi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "736ca316-d609-44f5-a9e6-808fa50606bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El nombre del lenguaje proviene de la afición de su creador por los humoristas británicos Monty Python.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ffb0c-356f-4d69-b028-465bb354c114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64728-664c-448c-ad2e-32ac9589d7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
