{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a5c49c-7ebf-4bd1-aeb6-a70225ff93fc",
   "metadata": {},
   "source": [
    "# Introducción al Manejo de Memoria en LangChain\n",
    "\n",
    "En LangChain, el manejo de memoria es un concepto clave que permite a las aplicaciones basadas en modelos de lenguaje realizar un seguimiento del historial de interacción. Esto resulta especialmente útil en escenarios donde el contexto de las interacciones anteriores debe ser conservado para proporcionar respuestas coherentes y relevantes.\n",
    "\n",
    "### ¿Qué es la Memoria en LangChain?\n",
    "\n",
    "El término \"memoria\" en LangChain se refiere a la capacidad de registrar y gestionar el historial de mensajes en una interacción, ya sea entre un usuario y un agente o en cualquier flujo conversacional. Este concepto es fundamental para aplicaciones como chatbots, asistentes virtuales y sistemas de soporte técnico, donde el contexto previo tiene un impacto significativo en la calidad de las respuestas generadas.\n",
    "\n",
    "### Tipos de Memoria en LangChain\n",
    "\n",
    "LangChain ofrece diferentes tipos de memoria, cada uno diseñado para adaptarse a necesidades específicas de almacenamiento y gestión del historial conversacional:\n",
    "\n",
    "- **ChatMessageHistory**:\n",
    "  - Permite guardar el historial de mensajes de un chat.\n",
    "  - Proporciona métodos como `add_user_message` y `add_ai_message` para registrar manualmente los mensajes de usuario y del modelo.\n",
    "--- \n",
    "- **ConversationBufferMemory**:\n",
    "  - Al utilizar cadenas de tipo `ConversationalChain`, guarda automáticamente todos los mensajes de la conversación en un objeto de memoria.\n",
    "  - Es ideal para escenarios donde es necesario conservar todo el historial de interacciones.\n",
    "---\n",
    "- **ConversationBufferWindowMemory**:\n",
    "  - Similar a `ConversationBufferMemory`, pero permite definir una ventana de tamaño `k` para almacenar solo las últimas `k` interacciones en lugar de todo el historial.\n",
    "  - Útil cuando se desea limitar la cantidad de contexto conservado para mejorar el rendimiento o reducir el consumo de recursos.\n",
    "---\n",
    "- **ConversationSummaryMemory**:\n",
    "  - En lugar de almacenar los mensajes completos, genera un resumen del historial de la conversación.\n",
    "  - Este enfoque reduce drásticamente el tamaño de la memoria, siendo ideal para conversaciones muy largas o aplicaciones con restricciones de almacenamiento.\n",
    "\n",
    "### Importancia del Manejo de Memoria\n",
    "\n",
    "El manejo de memoria en LangChain no solo permite mantener la coherencia en las respuestas, sino que también ayuda a optimizar los recursos y mejorar la eficiencia de las aplicaciones. La selección del tipo de memoria adecuado dependerá del caso de uso, el volumen de datos a manejar y los requiNotebook la aplicación.\n",
    "\n",
    "Este tutorial profundizará en cómo implementar y utilizar estos diferentes tipos de memoria en LangChain, brindándote las herramientas necesarias para construir aplicaciones más inteligentes y contextualmente conscientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec570b7-bbaf-48ad-a929-90e7a28af509",
   "metadata": {},
   "source": [
    "# Uso de ChatMessageHistory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1c09ba-ca5d-497a-9ff5-9af23013abf6",
   "metadata": {},
   "source": [
    "- Importamos las librerías relacionadas y conectamos con el LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aca75-5fa2-45f7-a6ca-7a1685a2b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Probar conexión\n",
    "respuesta = chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\")\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee9387-e638-4232-9bdf-2a494ce0ff3f",
   "metadata": {},
   "source": [
    "- Se instancia el objeto de histórico de mensajes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a6e72-54ac-4f78-967d-e9632c43c3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246de8f3-3e81-4a03-b406-00af0d94a7df",
   "metadata": {},
   "source": [
    "- Realizamos la consulta del usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada3f6d-3946-42b1-a352-58b187ce3ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = \"Hola, necesito asesoría para conectarme a la red wifi desde el PC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2b919-7c45-45d5-887c-a541236c4033",
   "metadata": {},
   "source": [
    "- Se deben ir almacenando en el objeto \"history\" los mensajes de usuario y los mensajes AI que queramos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dace7c-ce19-43f1-ae4d-7b81986d3e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_user_message(consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b60c9bf-af52-4e88-9e8b-324fafc5a682",
   "metadata": {},
   "source": [
    "- Realizamos la consulta al LLM para posteriormente también guardar la respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fd97a-8bfd-4a3d-ba79-88cd4668abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = chat.invoke([HumanMessage(content=consulta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6574bce5-9cd4-449f-8056-97a95dea4648",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resultado.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88e575b-03ce-4adf-afea-c1cbf3116634",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.add_ai_message(resultado.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06f02d-7360-4f8c-adf2-d86642e02060",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afc95f-d031-474a-b5b4-9f07aabc4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bc3cd-aac5-46f0-b91e-15db4c982108",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mensaje in history.messages:\n",
    "    print(f\"{mensaje.type}: {mensaje.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291716b1-5b4a-44bf-8be6-16fa168bf445",
   "metadata": {},
   "source": [
    "# Cómo crear un buffer de memoria de una conversación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157f712-39c4-4a0f-9b85-f3872d16dced",
   "metadata": {},
   "source": [
    "## Clases `ConversationChain` y `ConversationBufferMemory` en LangChain\n",
    "\n",
    "### `ConversationChain`\n",
    "La clase `ConversationChain` es una cadena predefinida en LangChain diseñada para gestionar conversaciones interactivas con modelos de lenguaje. Su propósito es facilitar la construcción de flujos de diálogo, incorporando el manejo de entradas del usuario, generación de respuestas por parte del modelo y la integración de memoria para mantener el contexto a lo largo de la conversación.\n",
    "\n",
    "#### Características principales:\n",
    "- Funciona como un flujo básico para aplicaciones conversacionales.\n",
    "- Es compatible con varios tipos de memoria para gestionar el historial de interacción.\n",
    "- Ideal para prototipos rápidos o casos simples donde se requiere una conversación con contexto.\n",
    "\n",
    "### `ConversationBufferMemory`\n",
    "La clase `ConversationBufferMemory` es un tipo de memoria que guarda el historial completo de la conversación en forma de mensajes. Se integra perfectamente con `ConversationChain` para asegurar que el modelo tenga acceso al contexto completo de las interacciones previas.\n",
    "\n",
    "#### Características principales:\n",
    "- Almacena todos los mensajes de la conversación en orden cronológico.\n",
    "- Útil para aplicaciones donde es crítico mantener el historial completo de interacciones.\n",
    "- Proporciona métodos simples para recuperar o actualizar el historial de mensajes.\n",
    "\n",
    "### Relación entre `ConversationChain` y `ConversationBufferMemory`\n",
    "`ConversationChain` puede utilizar `ConversationBufferMemory` como su componente de memoria, lo que permite gestionar conversaciones con contexto completo de manera sencilla y eficiente.\n",
    "\n",
    "Estas clases trabajan juntas para crear sistemas conversacionales que mantienen el historial y el contexto, mejorando la relevancia y coherencia de las respuestas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e68242-38a6-4e9f-ac20-d80b7cd0e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbd95c-fbd1-40ce-9532-4cd3063d29a0",
   "metadata": {},
   "source": [
    "- Instanciamos el un objeto de la clase **ConversationBufferMemory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b52680-8005-45bc-b97b-cac6f269dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1b1a6-42e3-4602-8e53-d1a45b53e3dd",
   "metadata": {},
   "source": [
    "- Se requiere para el ejemplo instanciar un objeto de la clase **ConversationChain**\n",
    "- En ella se instancia la cadena conversacional con el LLM y el objeto de memoria\n",
    "- Se mantiene verbose = True, para hacer seguimiento del proceso\n",
    "(Una alternativa reciente: con *RunnableWithMessageHistory:* https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c9d15-4c4d-4a80-a433-c8673542c582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation = ConversationChain(llm=chat,memory = memory,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86039a34-9441-457b-bd4f-26d98003ef29",
   "metadata": {},
   "source": [
    "- Se envía un primer prompt (human message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f66ff-c4c1-47ea-ab09-818b7ce6c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"Hola, Sabes que equipos llegaron a la final en mundial de fútbol de Mexico en 1986. Respondeme en español\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcf556-7f07-42bd-89a1-2f9c581edfe8",
   "metadata": {},
   "source": [
    "- Se envía un segundo prompt (human message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b310b5-2454-4714-a82c-5ed2a5c21cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"¿quienes eran los directores técnicos de cada selección?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472734a3-e582-4e86-8e00-ae27c5da41ca",
   "metadata": {},
   "source": [
    "- Se puede observar el **histórico** de la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d547b0ad-f0d0-42de-a8cf-7185f2f6ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3b7ef-1dbe-449f-b65e-c12f49ee160d",
   "metadata": {},
   "source": [
    "- Podemos cargar las variables historicas a la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb828dcf-b455-404e-a660-c0e51db0c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b56725-8bb5-421e-81be-84c1ff17cd8f",
   "metadata": {},
   "source": [
    "## Cómo salvar el buffer y posteriormente cargar "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0bfc72-0607-4abe-bb35-e0b0f86193b3",
   "metadata": {},
   "source": [
    "- Recordemos en donde está la memoria de la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cef97-a886-46af-846e-ffa680858d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9f1cb6-bab8-44e2-bb6f-59f3de7a2b98",
   "metadata": {},
   "source": [
    "### ¿Qué es la librería `pickle`?\n",
    "\n",
    "La librería `pickle` es un módulo estándar de Python que permite serializar y deserializar objetos. **Serializar** significa convertir un objeto de Python en una secuencia de bytes que puede ser almacenada en un archivo o transmitida a través de una red. **Deserializar** significa reconstruir el objeto original a partir de esa secuencia de bytes.\n",
    "\n",
    "### Usos principales\n",
    "- **Almacenamiento de datos**: Guardar estructuras de datos complejas, como listas, diccionarios o clases personalizadas, para su uso posterior.\n",
    "- **Transferencia de datos**: Enviar objetos entre diferentes sistemas o procesos.\n",
    "- **Persistencia de estados**: Guardar el estado de un programa o modelo para reanudarlo en otro momento.\n",
    "\n",
    "### Limitaciones\n",
    "- Solo es compatible con objetos de Python.\n",
    "- Puede ser inseguro si se carga un archivo pickle de una fuente no confiable, ya que podría ejecutar código malicioso.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44adf1-4934-4adb-af67-59c69c661117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Vamos a crear un objeto binario con todo el objeto de la memoria\n",
    "pickled_str = pickle.dumps(conversation.memory) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e7687-3c44-4eea-a687-57558f32343d",
   "metadata": {},
   "source": [
    "- Se cfrea un archivo binario para guardar la conversacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120246f-7314-4ad3-8055-4e5be1ebcb90",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Utilizamos wb para indicar que escriba un objeto binario, en la misma ruta que el script\n",
    "\n",
    "with open('memory.pkl','wb') as f:\n",
    "    f.write(pickled_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33ba1f-d2af-41b4-8a30-c4fc30f38ba6",
   "metadata": {},
   "source": [
    "### Cargar la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcabbd20-9a2a-42c3-b9a4-9674ce08890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos rb para indicar que leemos el objeto binario\n",
    "memoria_cargada = open('memory.pkl','rb').read() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f4195-4335-47ae-84fb-1d25e2f32b01",
   "metadata": {},
   "source": [
    "## Creamos una conversación nueva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798872c6-f06f-44aa-9e28-72c525a417bc",
   "metadata": {},
   "source": [
    "- Se creamos una nueva instancia de LLM para asegurar que está totalmente limpia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb687b3-c906-4cd8-95f8-2178e4b7c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear otra conexión con Groq\n",
    "chat2 = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc79f3-e1db-4124-96f1-a07edd3087f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversacion_recargada = ConversationChain(\n",
    "    llm=chat2, \n",
    "    memory = pickle.loads(memoria_cargada),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580d135-e057-431f-aad4-74c5614cf53b",
   "metadata": {},
   "source": [
    "- Vamos a verificar la memoria de la nueva conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cc84f-632d-48eb-87f7-bca7cb6a5e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversacion_recargada.memory.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655d4d3e-7d8e-4c36-9346-fc92c1351874",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿qué equipos participaron en las semifinales?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4081a1-a642-4bd5-bf3a-39dc7984ac88",
   "metadata": {},
   "source": [
    "## Crear un buffer con ventana de memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a54d6-722f-4bfd-9e24-9d3a4910d323",
   "metadata": {},
   "source": [
    "- Como siempre, carguemos las liberías involucradas y el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f12b9-41fc-4838-9403-25c8810d8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5092210-baea-42cf-bf6f-754fe5b316e2",
   "metadata": {},
   "source": [
    "- Se va a instanciar un objeto **ConversationBufferWindowMemory**, donde *k* indica el número de iteraciones (pareja de mensajes human-AI) que guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec310ba-26a2-476f-91fa-151107fc54c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba011cc-feab-40e8-98db-c448c16fe509",
   "metadata": {},
   "source": [
    "- Ahora se instancia  una cadena conversacional con el LLM y el objeto de memoria.\n",
    "\n",
    "    alternativa usar RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3af534-2779-4640-b7d2-9f69788ac465",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una instancia de la cadena conversacional con el LLM y el objeto de memoria\n",
    "conversation = ConversationChain(llm=chat,memory = memory,verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbd482-e88e-4acb-80a7-aaf9f252fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hola, soy Juan, ¿puedes ayudarme en algo?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8718532-5247-40d4-8dd8-4201065d72b2",
   "metadata": {},
   "source": [
    "- Continuamos con la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff43117-b3a6-4030-bf84-6be851d38d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Quiero hacer una consulta sobre la historia reciente de Colombia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d08b5ad-23fb-4004-9f78-88784f5b464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿Sabes que presidentes tuvo el pais entre 1980 y 1990?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c79017-93ba-4a7b-ba47-7ba9221aae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer) #k limita el número de interacciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eabf58-b779-4d53-952f-1fa3d1c2fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"¿A qué partidos políticos pertenecían dichos presidentes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166084e0-9307-4faf-ad01-1f73ace0cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer) #k limita el número de interacciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286ad22-b587-408e-8b73-92ad6ed7e035",
   "metadata": {},
   "source": [
    "## Manejar una conversación 'resumida' en la memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bd1411-a6e5-478e-b864-727d9b989c46",
   "metadata": {},
   "source": [
    "- Para lo cual vamos a usar la clase *ConversationSummaryBufferMemory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81559090-46ca-40d9-96bb-a180be2ad74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86702b-dfd9-4d32-817d-c2330d85be5b",
   "metadata": {},
   "source": [
    "- Instanciamos el objeto\n",
    "\n",
    "(Una versión actualizada: https://python.langchain.com/docs/versions/migrating_memory/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad6a41f-d537-4b9c-876d-f2a33b8b1bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si genera error, es posible requiera actualizar \n",
    "# pip install --upgrade langchain pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b7dac-d2fd-4bdf-a0c0-4cabbdadc1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0d307-506d-4487-8721-e986a26f859c",
   "metadata": {},
   "source": [
    "- **Creemos una plantilla para dar contexto a la conversaión**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751249a-0205-4c60-9fb7-24406eced401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Responde siempre en español.\"),\n",
    "    MessagesPlaceholder(\"history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2435291-f380-4164-ac88-23a27c4b6f5b",
   "metadata": {},
   "source": [
    "- Se crea un prompt cuya respuesta hará que se sobrepase el límite de tokens y por tanto sea recomendable resumir la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78915105-51ca-493c-8e18-134c5e9f45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "proceso_votacion = '''Según el reglamento académico, para sustentar el trabajo de grado en modalidad practica empresarial, se requiere certificado de la empresa de terminación de la práctica'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97983301-7c60-4850-ab18-6f83ee5db515",
   "metadata": {},
   "source": [
    "- Ahora se crea una conversación con memoria resumida\n",
    "\n",
    "Alternativa con RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414fc6b-916b-4357-82ec-4e8992e1cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=chat,\n",
    "    memory_key=\"history\",\n",
    "    return_messages=True,          # <<--- IMPORTANTE\n",
    "    max_token_limit=100\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a05c8c-3f2e-48bf-9518-cadece3da0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=chat,memory = memory, prompt = prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a12b13-4cbd-4d12-9a16-be422e418a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.invoke({\"input\":proceso_votacion})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebb69d-711f-40f5-b0eb-9f0398daddb3",
   "metadata": {},
   "source": [
    "- Revisemos que se está almacenando en la memoria de la conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85b264d-646f-4839-88e3-1e6b6fa28d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1ebf0-fb09-4283-9478-6c861c6c8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d64608-d83e-46ac-96d2-180a2b86ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta2 = \"¿para qué necesito ese certificado?\"\n",
    "respuesta = conversation.invoke(input=pregunta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdb2bd-24cf-402a-845a-79dbd4843792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb92c05-98dd-4f7f-b558-908097c966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1684f96f-d8d7-4e70-a3ae-04f7c5425900",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4c811-f080-40da-8a3e-fb1a371b8fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
