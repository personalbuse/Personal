{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b590e18-2ca1-4d6c-8f9a-f4325b773bff",
   "metadata": {},
   "source": [
    "# Cargadores: Integración con documentos otras plataformas\n",
    "\n",
    "\n",
    "\r\n",
    "LangChain ofrece una variedad de cargadores de documentos, incluyendo aquellos que se integran con plataformas externas. Estos cargadores de tipo \"integración\" funcionan de forma similar a los cargadores estándar, pero están diseñados para conectarse directamente con fuentes de datos externas.\r\n",
    "\r\n",
    "### Ejemplos de integraciones disponibles:\r\n",
    "\r\n",
    "- Plataformas de terceros como Google Cloud, AWS, Google Drive o Dropbox.\r\n",
    "- Bases de datos como MongoDB.\r\n",
    "- Sitios web específicos, como Wikipedia.\r\n",
    "- Fuentes no convencionales como videos de YouTube o conversaciones de WhatsApp.\r\n",
    "\r\n",
    "Estas integraciones permiten cargar contenido directamente desde múltiples fuentes para luego ser procesado por LangChain, lo cual es útil en aplicaciones como sistemas de preguntas y respuestas basados en videos, análisis de conversaciones y más.\r\n",
    "\r\n",
    "### Documentación oficial\r\n",
    "\r\n",
    "Puedes consultar el listado completo de cargadores con integración en:\r\n",
    "\r\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e650a-f903-4343-8530-f21af14f7ed5",
   "metadata": {},
   "source": [
    "## Iniciamos creando el objeto LLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "279c2d02-8217-4475-9a6e-e4858a420b47",
   "metadata": {},
   "source": [
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "#chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"mistral:latest\")\n",
    "chat = ChatOllama(model=\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac35cab-dcfb-46f1-961d-09a07cd41324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexion con OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env (busca automáticamente en el directorio actual o superiores)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró OPENAI_API_KEY en el .env ni en el entorno\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7df2d5-56f8-4f1e-83ec-29e581c3a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),   # sk-proj-...\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e929914-d9ed-4c81-99a8-d3c086a59e87",
   "metadata": {},
   "source": [
    "## Integración con Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f039fb3-f081-4d66-911d-154603b582ed",
   "metadata": {},
   "source": [
    "### WikipediaLoader (LangChain)\n",
    "\n",
    "Un *document loader* que carga artículos de **Wikipedia** como objetos `Document`. Configurable por idioma, cantidad máxima, metadatos y tamaño del contenido. Ideal para integraciones en flujos RAG\n",
    "\n",
    "---\n",
    "\n",
    "#### Uso\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"Machine learning\", lang=\"es\", load_max_docs=2)\n",
    "docs = loader.load()\n",
    "```\n",
    "## Parámetros principales\n",
    "\n",
    "- **`query`** *(str)* → término de búsqueda en Wikipedia.  \n",
    "- **`lang`** *(str, default=\"en\")* → idioma de la búsqueda.  \n",
    "- **`load_max_docs`** *(int, default=100, límite 300)* → número máximo de artículos a cargar.  \n",
    "- **`load_all_available_meta`** *(bool, default=False)* → si se incluye toda la metadata disponible.  \n",
    "- **`doc_content_chars_max`** *(int, default=4000)* → límite de caracteres por documento.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d3412-7580-41c2-b407-a068ea57b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia en una terminal\n",
    "from langchain.document_loaders import WikipediaLoader \n",
    "\n",
    "# Carguemos las demas clases requeridas de LangChain\n",
    "from langchain.prompts import (PromptTemplate, \n",
    "                                SystemMessagePromptTemplate, \n",
    "                                ChatPromptTemplate, \n",
    "                                HumanMessagePromptTemplate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db8d60-61ac-4dd9-bd9f-682ab5510674",
   "metadata": {},
   "source": [
    "### Crear el objeto de conexión y realizar la consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ead4f4-42fd-4110-a650-20127f609aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # obtener artículo de wikipedia\n",
    "personaje = \"Francisco de Paula Santander\"\n",
    "\n",
    "# Para ver los parámetros posibles en: https://python.langchain.com/v0.1/docs/integrations/document_loaders/wikipedia/\n",
    "docs = WikipediaLoader(query = personaje, lang = \"es\", load_max_docs = 10)  \n",
    "\n",
    "documentos_recuperados = docs.load()\n",
    "\n",
    "# para que sea más rápido solo pasamos el primer documento [0] como contexto extra\n",
    "contexto_extra = documentos_recuperados[0].page_content  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4590a32-4017-4fcb-a179-aa532495edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos_recuperados[0].page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f2851-9fc9-4102-a967-6fcb596d6058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pregunta usuario\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Responde a esta pregunta\\n{pregunta}, responde ESTRICTAMENTE sobre  el contenido extra dado a continuación:\\n{contenido}')\n",
    "\n",
    "# Construir prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "\n",
    "# Resultado\n",
    "pregunta_arg = \"¿En que fecha nacio Simón Bolivar?\"\n",
    "result = chat.invoke(chat_prompt.format_prompt(pregunta = pregunta_arg, contenido = contexto_extra).to_messages())\n",
    "    \n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133ea87-d137-4293-9867-d4232f23eb2e",
   "metadata": {},
   "source": [
    "- **Veamos el contexto extra que ha tenido el LLM como base:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ee9d8-d0c1-4cd8-bb76-18c7edaa47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contexto_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30265ff-a96c-48af-8d13-b75e38179cb0",
   "metadata": {},
   "source": [
    "## Conexión con basa de datos Artículos Científicos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944e8df-ce71-4520-99e0-aa1724691e34",
   "metadata": {},
   "source": [
    "\n",
    "El **`ArxivLoader`** es un *document loader* de LangChain que permite buscar y cargar artículos científicos desde **arXiv**.  \n",
    "\n",
    "- **Búsqueda**: acepta consultas por palabra clave, categorías (`cs.AI`, `stat.ML`, etc.) o ID de paper.  \n",
    "- **Salida**: devuelve una lista de objetos `Document`.  \n",
    "  - `page_content`: texto del paper (abstract o PDF completo si está disponible).  \n",
    "  - `metadata`: incluye título, autores, fecha de publicación, categorías, URL del PDF y el **abstract** oficial (`Summary`).  \n",
    "- **Parámetros clave**:  \n",
    "  - `query` → término de búsqueda.  \n",
    "  - `load_max_docs` → número máximo de resultados.  \n",
    "\n",
    "Ideal para integrar papers en flujos de **RAG** o para análisis de literatura científica en proyectos de investigación.\n",
    "\n",
    "El manual de usuario de la API de ArXiv aqui: https://info.arxiv.org/help/api/user-manual.html\n",
    "El manual de cargador aqui: https://python.langchain.com/docs/integrations/document_loaders/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a53d7-7642-4de2-9f8d-677378b30dec",
   "metadata": {},
   "source": [
    "### Ejemplo:\n",
    "- Buscar 3 papers en arXiv y mostrar la metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797fa2c-8754-43d9-b018-d7f27b358bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instalar por consola: \n",
    "# pip install  arxiv\n",
    "# pip install pymupdf\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# 1) Configura tu búsqueda (palabras clave o categorías de arXiv, p. ej. \"cs.AI\")\n",
    "query = \"cs.AI and transformers for time series\"\n",
    "n_results = 3\n",
    "\n",
    "# 2) Carga resultados desde arXiv (abstracts + metadatos)\n",
    "loader = ArxivLoader(query=query, load_max_docs=n_results)\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6044dba6-95bc-4ffb-84f4-f7d3c6adae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d402f-1a70-47bf-ab5a-a5df54268492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requiere: pip install langchain langchain-community arxiv pymupdf \n",
    "\n",
    "import textwrap  # Para dar formato a cadenas de texto largas\n",
    "\n",
    "def limpia(texto: str) -> str:\n",
    "    # Quita saltos de línea raros y espacios dobles\n",
    "    return \" \".join(texto.split())\n",
    "\n",
    "for i, d in enumerate(docs[:n_results], start=1):\n",
    "    m = d.metadata or {}\n",
    "    titulo = m.get(\"Title\", \"N/A\")\n",
    "    autores = m.get(\"Authors\", \"N/A\")\n",
    "    anio = (m.get(\"Published\") or \"N/A\")[:4]\n",
    "    abstract = m.get(\"Summary\") or d.page_content  # prefiero el abstract oficial\n",
    "\n",
    "    print(f\"\\n=== Documento {i} ===\")\n",
    "    print(f\"Título : {titulo}\")\n",
    "    print(f\"Autores: {autores}\")\n",
    "    print(f\"Año    : {anio}\")\n",
    "    print(\"\\nAbstract:\")\n",
    "    print(textwrap.fill(limpia(abstract), width=100))  # Escribe lineas de maximo 100 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee73fdd-2d4d-46ba-9e86-51b8cc81740f",
   "metadata": {},
   "source": [
    "# Transformación de documentos\n",
    "\n",
    "\r\n",
    "Una vez que se carga un documento desde una fuente externa mediante un cargador, se obtiene un objeto de tipo `Document`, cuyo contenido principal se encuentra en el campo `page_content`.\r\n",
    "\r\n",
    "### División en fragmentos (chunks)\r\n",
    "\r\n",
    "En muchos casos, el texto dentro de `page_content` puede ser demasiado extenso para ser procesado directamente por un modelo de lenguaje, ya que estos modelos suelen tener un límite en la cantidad de tokens que pueden manejar (por ejemplo, unos 8,000 tokens, equivalentes a unas 6,000 palabras aproximadamente).\r\n",
    "\r\n",
    "Para resolver este problema, LangChain proporciona transformadores de documentos. Estos permiten dividir el contenido de `page_content` en fragmentos más pequeños, llamados *chunks*, de manera automática y controlada.\r\n",
    "\r\n",
    "### Utilidad de los fragmentos\r\n",
    "\r\n",
    "Estos fragmentos tienen múltiples usos. Uno de los más importantes es la conversión de cada fragmento en un vector numérico mediante un proceso de incrustación (*embedding*). Estos vectores pueden almacenarse y luego utilizarse para realizar búsquedas eficientes basadas en similitud.\r\n",
    "\r\n",
    "Por ejemplo, si estamos construyendo una aplicación de preguntas y respuestas basada en documentos, los vectores de cada fragmento permitirán encontrar rápidamente el contenido más relevante para responder una consulta, sin necesidad de recorrer todo el contenido de manera secuencial.\r\n",
    "\r\n",
    "Esta estrategia mejora significativamente la eficiencia y precisión cuando se necesita proporcionar contexto adicional a un modelo de lenguaje grande (LLM).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df985a0c-57d4-4421-91cc-9e63ab240048",
   "metadata": {},
   "source": [
    "#  Transformadores de documentos en LangChain\n",
    "\n",
    "| Categoría                     | Clase / Ejemplo                               | Uso principal                                                                 |\n",
    "|--------------------------------|-----------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Text Splitters**            | `CharacterTextSplitter`                      | Divide texto por separadores simples (ej. saltos de línea, espacios).          |\n",
    "|                                | `RecursiveCharacterTextSplitter`              | Divide jerárquicamente (párrafos → frases → palabras), el más recomendado.     |\n",
    "|                                | `TokenTextSplitter`                          | Divide en chunks según número de *tokens* (ej. compatible con OpenAI).        |\n",
    "|                                | `MarkdownTextSplitter`                       | Divide preservando la estructura de encabezados Markdown.                      |\n",
    "|                                | `LatexTextSplitter`                          | Especializado en documentos LaTeX.                                            |\n",
    "|                                | `HTMLHeaderTextSplitter`                     | Divide respetando encabezados HTML (h1, h2, etc.).                            |\n",
    "|                                | `PythonCodeTextSplitter`, `JavascriptSplitter` | Segmentación de código por funciones, clases o bloques.                        |\n",
    "| **Metadata & Filters**        | `EmbeddingsRedundantFilter`                  | Elimina chunks redundantes usando similitud de embeddings.                     |\n",
    "|                                | `EmbeddingsClusteringFilter`                 | Agrupa chunks en *clusters* semánticos.                                       |\n",
    "|                                | `EmbeddingsFilter`                           | Filtra documentos según relevancia semántica.                                 |\n",
    "|                                | `LLMChainFilter`                             | Usa un LLM para decidir qué documentos conservar.                             |\n",
    "| **Document Compressors**      | `DocumentCompressorPipeline`                 | Encadena pasos de compresión y filtrado de documentos.                        |\n",
    "|                                | `LLMChainExtractor`                          | Usa un LLM para extraer información clave y reducir texto.                    |\n",
    "| **Summarization Chains**      | `StuffDocumentsChain`                        | Une todos los documentos en un solo prompt.                                   |\n",
    "|                                | `MapReduceDocumentsChain`                    | Resumen en dos fases: map (local) y reduce (global).                          |\n",
    "|                                | `RefineDocumentsChain`                       | Construye resúmenes incrementales, refinando cada paso.                       |\n",
    "| **Custom Transformers**       | `BaseDocumentTransformer` (clase base)       | Crear tus propios transformadores: limpieza, normalización, extracción, etc.  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af707f-b79a-423d-8150-cd47fa9cc648",
   "metadata": {},
   "source": [
    "## Cargar Fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e322b3-e5a2-4532-89f6-c5bcb923c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datos/ReglamentoEstudiantil.txt', encoding= \"utf8\") as file:\n",
    "    texto_completo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575affc-2eed-4348-9bb6-ba771fc8b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de caracteres\n",
    "len(texto_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82825491-fd2d-4279-ac1d-1e1f21aecc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de palabras\n",
    "len(texto_completo.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735401c-6ea0-4478-b329-c2215fd7641d",
   "metadata": {},
   "source": [
    "## Transformador \"CharacterTextSplitter\"\n",
    "\r\n",
    "\r\n",
    "Uno de los transformadores más utilizados en LangChain para dividir texto en fragmentos es `CharacterTextSplitter`. Este componente permite separar el contenido de un documento (`page_content`) en bloques más pequeños basados en un número específico de caracteres.\r\n",
    "\r\n",
    "Esta división es útil cuando el texto original es demasiado largo para ser procesado por completo por un modelo de lenguaje, ya que permite mantener fragmentos manejables en tamaño, sin perder el orden ni el contexto general.\r\n",
    "\r\n",
    "`CharacterTextSplitter` permite ajustar parámetros como:\r\n",
    "- `chunk_size`: la longitud máxima de cada fragmento.\r\n",
    "- `chunk_overlap`: la cantidad de caracteres que se solapan entre fragmentos consecutivos (útil para preservar contexto entre divisiones).\r\n",
    "\r\n",
    "Es una herramienta simple pero poderosa para preparar documentos antes de realizar tareas como incrustación (embeddings), búsqueda semántica o consultas con LLMs.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6ecec8-1866-4353-b104-f637f5c1d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0faeec-db86-4b19-b5f7-b780994dbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Se indica que se divida en párrafos de tamaño de alrededor de 1000 caracteres\n",
    "text_splitter = CharacterTextSplitter(separator = '\\n', chunk_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc3789-018a-467f-b1dc-b0703d719d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea el documento utilizando el transformador\n",
    "textos = text_splitter.create_documents([texto_completo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d99d7-c1ec-4a18-8145-e80ac92cdb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(textos)) # Verficamos el tipo del objeto obtenido\n",
    "print('\\n')\n",
    "print(type(textos[0]))  # Verificamos el tipo de cada elemento\n",
    "print('\\n')\n",
    "print(textos[2])\n",
    "#print(textwrap.fill(textos[2].page_content, width=80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a09543-3d6d-4add-bce5-600931c31131",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490af4f9-5173-43af-aee7-e938d5b97813",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textos[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d77ad6-b732-4e1e-a19c-2662ed1045ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3d7f0-0eeb-4698-93d0-bac5852ca725",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(textos[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18802fca-920a-48e8-8b35-3cff39f6aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "textos[1].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f51c8e-05c8-4b20-84b6-2466c2bfd171",
   "metadata": {},
   "source": [
    "### Y si esta en pdf ( ejemplo anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e52e1-f27a-4c53-86b1-a047cbf7874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34ab1da-bd25-43f6-8963-344c60721774",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('Datos/ReglamentoEstudiantil.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ef4bb-e4ee-4a6d-86b0-206506445b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50, separator=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f2cfb0-c19c-4558-838e-ab1b8d70d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split(text_splitter=splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abe2104-a22f-4351-ac56-afc768a4c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d2da32-a033-4244-bc73-381f229461b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d570d6e-8a1c-4014-8fe0-867e22b6ece4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7657c2-8f62-4b28-b15c-2e0e823c15d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8babc21a-74e2-49fd-a92e-195b8d3cff97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9bfd92-96ae-4841-bf0a-50bf9400520f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
