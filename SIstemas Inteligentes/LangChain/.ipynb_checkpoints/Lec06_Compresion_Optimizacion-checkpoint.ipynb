{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02362fb4-e9ce-44c6-a990-742f8fbd2360",
   "metadata": {},
   "source": [
    "# Compresión y optimización de datos obtenidos en BD mediante LLM's\n",
    "\n",
    "\r\n",
    "En un flujo típico de trabajo con modelos de lenguaje y documentos largos, se pasa por varias etapas antes de obtener una respuesta útil y precisa. Uno de los pasos más importantes es **optimizar o comprimir la información recuperada** para que el modelo pueda utilizarla de manera efectiva.\r\n",
    "\r\n",
    "### Flujo general de trabajo\r\n",
    "\r\n",
    "1. **Carga del documento**\r\n",
    "   - Se utiliza un loader para cargar archivos desde diversas fuentes de datos (PDF, CSV, Google Drive, etc.).\r\n",
    "\r\n",
    "2. **División en fragmentos (chunks)**\r\n",
    "   - Los documentos se dividen en fragmentos de texto manejables mediante transformadores como `CharacterTextSplitter`.\r\n",
    "\r\n",
    "3. **Generación de vectores**\r\n",
    "   - Cada fragmento se convierte en un vector numérico utilizando una función de *embedding*. Estos vectores representan el significad\n",
    "\n",
    "\n",
    "4. **Almacenamiento en una base de datos vectorial**\r\n",
    "- Los vectores generados se almacenan en una Vector DB (como FAISS, Chroma o SKLearnVectorStore) para realizar búsquedas por similitud.\r\n",
    "\r\n",
    "5. **Búsqueda por similitud**\r\n",
    "- Ante una consulta del usuario, se genera un vector y se comparan distancias con los vectores almacenados.\r\n",
    "- Se devuelve un *ranking* con los fragmentos más similares.\r\n",
    "\r\n",
    "6. **Compresión u optimización con un LLM**\r\n",
    "- En lugar de pasar toda la información recuperada al modelo, se puede utilizar el LLM para generar una **respuesta más corta y rel\n",
    "  evante**.\r\n",
    "- Este paso no es una compresión en el sentido técnico de reducir tamaño de archivo, sino en el sentido semántico: el LLM sintetiza el contenido para dar una **respuesta más enfocada, útil y clara**.\r\n",
    "\r\n",
    "### ¿Por qué es importante esta optimización?\r\n",
    "\r\n",
    "- **Limita la cantidad de tokens que se envían al modelo**, lo cual puede afectar el costo y rendimiento.\r\n",
    "- **Mejora la precisión y relevancia de la respuesta final**, al evitar que el modelo se \"distraiga\" con información irrelevante.\r\n",
    "- **Permite construir asistentes más eficientes**, como chatbots que respondan basándose solo en los fragmentos más significativos del documento original.\r\n",
    "\r\n",
    "Este enfoque es especialmente útil para aplicaciones de recuperación aumentada por generación (*Retrieval-Augmented Generation* o RAG), donde se utiliza información externa para mejorar las respuestas de los modelos de lenguaje.\r\n",
    "oo de vectores:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bba99-c0f1-4fb1-94f5-87173a1f53e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las lirerías\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import SKLearnVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f8d65-e517-4c0d-9c79-3c88f17155c2",
   "metadata": {},
   "source": [
    "## Carga documentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc698049-886f-481e-b89c-033a1d57e2f0",
   "metadata": {},
   "source": [
    "- Vamos a realizar una consulta puntual sobre Python, para lo cual, iniciamos cargando la información disponible en wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c031a92-1b0a-4789-9a2e-de5d05207bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WikipediaLoader(query = 'Lenguaje Python', lang = 'es')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3310948-09dc-4b73-afcb-86df28802f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f4431e-ecfb-40d3-8882-92919ef9cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b83c1-f157-4fd8-b70f-066fb3aa24eb",
   "metadata": {},
   "source": [
    "## Split de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7fa30-3efc-42f0-abe7-ad6e020853cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04756f75-22f9-49fa-8a40-c241e095b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4937165a-9355-46f4-96be-86df3f55eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50948368-187e-4acb-bb34-96e22b08eb66",
   "metadata": {},
   "source": [
    "## Cargar el modelo embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4361bbe-591f-40e7-a1a3-0e62dfe60838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Detectar GPU si está disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instanciar LaBSE\n",
    "funcion_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/LaBSE\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba646959-d831-4131-be2e-5f6aee56eb0c",
   "metadata": {},
   "source": [
    "## Incustar documentos en BD de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbfdab-4c70-4d38-af65-af504cc72548",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"./ejemplo_wiki_bd\"  # ruta donde se guarda la BD vectorizada\n",
    "\n",
    "# Se crea la BD de vectores a partir de los documentos y la función de embeddings\n",
    "vector_store = SKLearnVectorStore.from_documents(\n",
    "    documents = docs,\n",
    "    embedding = funcion_embedding,\n",
    "    persist_path = persist_path,\n",
    "    serializer = \"parquet\"\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda74841-461a-4258-b64a-1fbbcf51713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9c6739-18c0-4c0d-bef7-cacb942a987f",
   "metadata": {},
   "source": [
    "## Consulta normal similitud del coseno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220b9401-4daa-470d-a99c-b6f21bb79ac1",
   "metadata": {},
   "source": [
    "- **Creamos un nuevo documento que será la consulta para buscar la mayor similitud en la BD usando la distancia del coseno**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f254eb1-a522-4ac5-9fd4-9372a1acea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "consulta = \"¿A que se debe el nombre del Lenguaje de programación Pyhton?\"\n",
    "docs = vector_store.similarity_search(consulta)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3a85e-adb6-436b-a633-ed664c142ec4",
   "metadata": {},
   "source": [
    "\n",
    "## Compresión Contextual de Documentos con LangChain y LLM´s\r\n",
    "\r\n",
    "Cuando realizamos una búsqueda en una base de datos vectorial, los resultados pueden incluir múltiples fragmentos de texto relevantes. Sin embargo, no siempre es eficiente pasar todo ese contenido al modelo. Para resolver esto, LangChain ofrece herramientas que permiten **comprimir y optimizar** esos fragmentos antes de enviarlos al LLM.\r\n",
    "\r\n",
    "Dos componentes clave para esto son:\r\n",
    "\r\n",
    "### `ContextualCompressionRetriever`\r\n",
    "\r\n",
    "Esta clase actúa como un *envoltorio* (wrapper) alrededor de otro retriever. Su objetivo es mejorar los resultados originales aplicando un proceso de compresión o filtrado adicional. Lo que hace es:\r\n",
    "\r\n",
    "- Recuperar documentos normalmente desde una base vectorial.\r\n",
    "- Pasar esos documentos a un *compresor* (por ejemplo, un LLM).\r\n",
    "- Devolver solo la versión comprimida o más relevante al modelo final.\r\n",
    "\r\n",
    "Se usa para reducir la cantidad de texto innecesario que se le envía al LLM, optimizando tanto el rendimiento como la calidad de las respuestas.\r\n",
    "\r\n",
    "### `LLMChainExtractor`\r\n",
    "\r\n",
    "Este es un tipo de *compresor de documentos* que utiliza un modelo de lenguaje (LLM) para extraer la parte más relevante de cada fragmento. En lugar de devolver el texto completo, devuelve una versión resumida, filtrada o ajustada del contenido.\r\n",
    "\r\n",
    "- Funciona como una cadena (`Chain`) de LangChain con un prompt predefinido.\r\n",
    "- Es útil para cuando se necesita pasar solo el *contexto esencial* al modelo, especialmente en aplicaciones tipo pregunta-respuesta (Q&A).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### ¿Cómo se usan juntos?\r\n",
    "\r\n",
    "Ambas clases se combinan para construir un flujo donde se recuperan documentos y luego se comprimen antes de enviarlos al modelo:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a970447-a239-41cf-8997-8f63c3aa398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4d4bb-c6da-4fd3-ad92-492b134ba16f",
   "metadata": {},
   "source": [
    "- **Conectamos con el modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5883da7a-0f6b-4e80-9b56-dd0b0a0b835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo documento que será la consulta para buscar la mayor similitud en la BD usando la distancia del coseno\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Probar conexión\n",
    "respuesta = chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\")\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02f1ee-29da-4392-99f5-69aaf05168b9",
   "metadata": {},
   "source": [
    "- **Instanciamos el compressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fa2175-7b82-4cfb-9cef-4dd859df2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressor = LLMChainExtractor.from_llm(chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e791e5e7-843c-49c0-9eab-b0c3b1abd4ac",
   "metadata": {},
   "source": [
    "- **Se instancia el recuperador, dando como argumentos el compressor y la base de datos donde se buscara**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d182bb22-5093-4152-b202-1b6eede25aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(base_compressor = compressor, base_retriever = vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e7d21-4fff-4b15-867f-da2d3422d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Respuesta_comprimida = compression_retriever.invoke(\"¿A que se debe el nombre del Lenguaje de programación Pyhton?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ca316-d609-44f5-a9e6-808fa50606bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Respuesta_comprimida[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ffb0c-356f-4d69-b028-465bb354c114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa64728-664c-448c-ad2e-32ac9589d7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
