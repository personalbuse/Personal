{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "178c06ca-c915-4eb4-bba1-c308f571d5ad",
   "metadata": {},
   "source": [
    "# RAG: Retrieval-Augmented Generation\n",
    "\n",
    "**RAG (Generación Aumentada por Recuperación)** es una técnica que combina **modelos de lenguaje (LLMs)** con **motores de recuperación de información**.  \n",
    "Su objetivo es permitir que el modelo genere respuestas más precisas y actualizadas al **recuperar documentos relevantes** desde una base de datos, un buscador o un sistema de conocimiento, antes de producir la salida final.\n",
    "\n",
    "###  Funcionamiento básico:\n",
    "1. **Consulta del usuario** → se convierte en un *query*.  \n",
    "2. **Recuperación** → se buscan documentos relevantes en una base vectorial o índice semántico.  \n",
    "3. **Generación** → el LLM recibe tanto la consulta como los documentos recuperados y genera una respuesta más confiable.  \n",
    "\n",
    "###  Ventajas:\n",
    "- Reduce **alucinaciones** del modelo.  \n",
    "- Permite respuestas basadas en **información actualizada**.  \n",
    "- Facilita integrar **bases de conocimiento privadas o especializadas**.  \n",
    "\n",
    "Ejemplo de aplicación: chatbots con información de manuales internos, sistemas de soporte técnico, búsqueda académica, etc.\n",
    "\n",
    "La imagen siguiente ilustra el proceso:\n",
    "\n",
    "<img src=\"imgs/RAG.png\" alt=\"Diagrama RAG\" width=\"800\"/>\n",
    "\n",
    "Un elemento muy importante a la hora de diseñar los sistemas RAG, son los embeddings. A continuación hacemos una instroducción a su uso dentro de la linea que hemos venido desarrollando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f23b4d-df19-48c4-81fb-845faced9ce6",
   "metadata": {},
   "source": [
    "## Incrustaciones mediante embedings\n",
    "\n",
    "\n",
    "Una vez que los documentos han sido divididos en fragmentos con herramientas como `CharacterTextSplitter`, el siguiente paso común en un flujo de trabajo con LangChain es generar *incrustaciones* o *embeddings*.\n",
    "\n",
    "Los embeddings son representaciones numéricas (vectores) de cada fragmento de texto. Estas representaciones permiten comparar y buscar similitudes entre fragmentos de manera eficiente utilizando operaciones matemáticas.\n",
    "\n",
    "Este proceso es esencial para construir aplicaciones como chatbots con recuperación de contexto, motores de búsqueda semántica o sistemas de preguntas y respuestas basados en documentos. Cada fragmento, ahora convertido en vector, puede almacenarse en una base de datos vectorial y recuperarse rápidamente según su similitud con una consulta dada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00389dfd-1eb6-42e4-b035-0aa33fc74ca3",
   "metadata": {},
   "source": [
    "## Embedding Propietarios y Open Source. \n",
    "\n",
    "### - Importación de clases para modelos open source o comerciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605e3d2-3e9d-4a2d-a176-3fdf221892e3",
   "metadata": {},
   "source": [
    "#### Importación de clase sobre las librerías de OpenAI para LLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "382ca61c-8a56-4d6e-9c81-319b4704b5b8",
   "metadata": {},
   "source": [
    "# Habilite como code, y dessabilite la instanciacion de los otros modelos, dejandolo en modo raw\n",
    "\n",
    "# Conexion con OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env (busca automáticamente en el directorio actual o superiores)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró OPENAI_API_KEY en el .env ni en el entorno\")\n",
    "\n",
    "# Crear instancia del modelo\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")  # sk-proj-...\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key= api_key,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe678f2-a056-4942-9927-ece2c0b51309",
   "metadata": {},
   "source": [
    "#### Importación de clases sobre las librerias de Ollama para LLM"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72b5bde2-da0f-4690-85f3-e064ded57b24",
   "metadata": {},
   "source": [
    "# Habilite como code, y dessabilite la instanciacion de los otros modelos, dejandolo en modo raw\n",
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "#chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"mistral:latest\")\n",
    "#chat = ChatOllama(model=\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9614d-5ec9-4225-8cde-6b6499c0b93e",
   "metadata": {},
   "source": [
    "### Importación de clases para conectar con Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09a712-a385-4628-8801-66a2c5fcfbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama3-70b-8192\",   # También puedes usar \"mixtral-8x7b-32768\"\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Probar conexión\n",
    "respuesta = chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\")\n",
    "print(respuesta.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e219d-adbc-4dd7-952e-210336afa28b",
   "metadata": {},
   "source": [
    "##  Incrustación de texto (embedding) de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb32501-d221-4be1-ac4e-af137c641338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e91a808-398a-4376-aa73-042b34db21c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = OpenAIEmbeddings(openai_api_key = api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d6839-b5b4-49f4-881d-f50f8f9b2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto = \"Esto es un texto enviado a OpenAI para ser incrustado en un vector n-dimensonal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bd29ad-e6dc-4d34-9614-106fd2558a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_text = embeddings.embed_query(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8b1662-1331-48ef-8ab9-ba848e2816ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3604f24f-4383-4874-a2fc-c2e71825eddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fe993f-1c7f-432a-9dc4-1bbcae919473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3d7251-73c2-4130-9883-8db8b17dce1c",
   "metadata": {},
   "source": [
    "##  Incrustación de texto (embedding) con MiniLM Multilingüe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705de238-0aee-4d83-9d46-3a076fd1fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar LangChain HuggingFace wrapper (para integrar embeddings open-source en LangChain):\n",
    "#pip install -U langchain-huggingface\n",
    "\n",
    "#instalar: Sentence Transformers (librería base para embeddings como MiniLM, mE5, LaBSE, etc.):\n",
    "#pip install -U sentence-transformers\n",
    "\n",
    "# PyTorch (el motor de deep learning que ejecuta los modelos): Si solo se dispone de CPU:\n",
    "#pip install torch\n",
    "\n",
    "# Si se dispone de GPU (CUDA), se debe instalar la versión adecuada según el driver. Ejemplo para CUDA 12.1:\n",
    "# pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
    "\n",
    "# Este ultimo suele ser mas facil con conda:\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia)\n",
    "\n",
    "# Opcional, para mejorar las descargar de HugginFace\n",
    "# pip install \"huggingface_hub[hf_xet]\"\n",
    "\n",
    "# Ver widgets como barra de progreso\n",
    "# pip install -U ipywidgets\n",
    "\n",
    "#from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465e0f8-d250-40b5-988c-6b748e1aa450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con este código se puede verificar si está instalada la GPU\n",
    "\n",
    "import torch\n",
    "print(\"Versión Torch:\", torch.__version__)\n",
    "print(\"CUDA disponible:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0a8ad-0f42-4fb5-8ed5-ad8e34eb6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b065d-6cfd-4fb6-b952-ba81dd262c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"La inteligencia artificial está transformando la educación.\"\n",
    "vec = embeddings.embed_query(texto)\n",
    "\n",
    "print(\"Dim:\", len(vec), \"| primeros 8:\", vec[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec2b46-d89f-4e5b-898b-de7d29b75ffd",
   "metadata": {},
   "source": [
    "##  Incrustación de texto (embedding) con mE5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf860aa-d3ab-491c-9d46-6fd337e61776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Detectar GPU si está disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instanciar el modelo (puedes cambiar entre \"base\" y \"large\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-base\",   # o \"intfloat/multilingual-e5-large\"\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad592f4-1bc2-49b4-9844-d81ab5ba952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto de prueba (en español)\n",
    "texto = \"La inteligencia artificial está transformando la educación.\"\n",
    "vec = embeddings.embed_query(texto)\n",
    "\n",
    "print(\"Dimensión del embedding:\", len(vec))\n",
    "print(\"Primeros 8 valores:\", vec[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a95b1a3-82c9-4357-877b-1adc78d3f893",
   "metadata": {},
   "source": [
    "##  Incrustación de texto (embedding) con LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a358b-cfc6-490f-ade6-07b416360495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Detectar GPU si está disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instanciar LaBSE\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/LaBSE\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85671d39-e2d5-4bc5-afc2-7d963e4e01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texto en español\n",
    "texto = \"La inteligencia artificial está transformando la educación.\"\n",
    "vec = embeddings.embed_query(texto)\n",
    "\n",
    "print(\"Dimensión del embedding:\", len(vec))\n",
    "print(\"Primeros 8 valores:\", vec[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889207a-5b1f-4078-9110-918af8e05238",
   "metadata": {},
   "source": [
    "# Ejemplo: Contruyendo un Mini Sistema RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594f5dd-3048-4da0-b94f-c7bd14f8fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86208fa1-4fc4-44a2-a021-1b29a7994b5a",
   "metadata": {},
   "source": [
    "## Creando la Base De Datos vectorial\n",
    "\n",
    "- Vamos a cargar los documentos, dividirlos en segmentos (chuncks) y codificarlos en embeddings. Luego almacenarlos en una base de datos vectorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e182bd6c-7294-41fd-88f8-a1806e3022d9",
   "metadata": {},
   "source": [
    "### 1. Cargar y dividir el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968931e3-de4f-4ee8-8435-4c04b7638da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = TextLoader('Datos/ReglamentoEstudiantil.txt', encoding = \"utf8\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27774b33-20cf-4c77-bc6d-ab494acf44de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir en chunks\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size = 500) # Otro método de split basado número de tokens\n",
    "documentos = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd088f-a915-4b38-93d0-70850a2e1e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documentos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa688c-3b5c-43ab-83a8-6b745924c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documentos[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8ff129-3a7f-4538-8d92-2eac251661a1",
   "metadata": {},
   "source": [
    "### 2. Usaremos embeddings Open Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfc822-95c0-4e40-b68d-c19e2ca21eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "funcion_embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/LaBSE\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs={\"normalize_embeddings\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a8ca7-0ff6-48f0-82eb-7f87ac87f21a",
   "metadata": {},
   "source": [
    "## Bases de Datos Vectoriales\n",
    "\n",
    "Después de generar embeddings a partir de fragmentos de texto, es necesario almacenarlos de manera eficiente para poder realizar búsquedas basadas en similitud. Para esto se utilizan las bases de datos vectoriales.\n",
    "\n",
    "A diferencia de las bases de datos tradicionales, estas están diseñadas para manejar vectores y permiten realizar operaciones como \"buscar el vector más parecido a este otro\", lo cual es fundamental en aplicaciones como:\n",
    "\n",
    "- Búsquedas semánticas\n",
    "- Chatbots con recuperación de contexto\n",
    "- Recomendadores\n",
    "- Análisis de similitud entre textos\n",
    "\n",
    "### Bases de datos vectoriales conocidas\n",
    "\n",
    "Algunas de las más utilizadas en el ecosistema de IA son:\n",
    "\n",
    "- **FAISS** (Facebook AI Similarity Search): muy rápida y eficiente, ideal para producción.\n",
    "- **Chroma**: ligera y fácil de usar, integrada con LangChain.\n",
    "- **Pinecone**: servicio en la nube con capacidades avanzadas de escalabilidad.\n",
    "- **Weaviate** y **Milvus**: orientadas a grandes volúmenes de datos y con funcionalidades de búsqueda híbrida.\n",
    "\n",
    "### Alternativa local: SKLearnVectorStore\n",
    "\n",
    "Para entornos locales, pruebas rápidas o proyectos pequeños, LangChain ofrece una alternativa sencilla llamada `SKLearnVectorStore`. Utiliza el algoritmo `NearestNeighbors` de Scikit-learn para realizar búsquedas de similitud entre los vectores generados.\n",
    "\n",
    "Aunque no está pensada para entornos de producción, es una excelente opción para experimentar en notebooks sin depender de servicios externos ni base\n",
    "## ¿Qué es Parquet?\n",
    "\n",
    "Parquet es un formato de almacenamiento de datos en columnas, optimizado para trabajar con grandes volúmenes de información de manera eficiente. Fue desarrollado por Apache como parte del ecosistema Hadoop, pero hoy en día es ampliamente usado en proyectos de ciencia de datos, big data y aprendizaje automático.\n",
    "\n",
    "### Características principales\n",
    "\n",
    "- **Formato columnar**: en lugar de almacenar los datos fila por fila (como un CSV), almacena cada columna por separado. Esto permite acceder rápidamente a columnas específicas sin tener que leer todo el archivo.\n",
    "- **Compresión eficiente**: al agrupar datos similares (por columna), logra una compresión muy efectiva, reduciendo el tamaño del archivo.\n",
    "- **Compatible con pandas y PyArrow**: puedes leer y escribir archivos Parquet fácilmente en Python usando bibliotecas como `pandas` y `pyarrow`.\n",
    "\n",
    "### ¿Por qué usar Parquet?\n",
    "\n",
    "- Es ideal para almacenar datasets grandes de forma compacta.\n",
    "- Reduce el tiempo de lectura y escritura, especialmente cuando se trabaja con subconjuntos de columnas.\n",
    "- Es compatible con muchas plataformas de procesamiento de datos como Spark, Hive, Dask, DuckDB, entre otras.\n",
    "s de datos adicionales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5404d5-f757-4278-bed5-78f74d2db163",
   "metadata": {},
   "source": [
    "### 3. Codificamos el texto en embeddings y lo almacenamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1d8e3-a5f2-4100-b2c7-b603893d9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar con pip install scikit-learn  \n",
    "# pip install pandas pyarrow\n",
    "\n",
    "from langchain_community.vectorstores import  SKLearnVectorStore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff69a40-07b5-4907-85cf-d99dbdb2d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_path = \"ejemplosk_embedding_db\"  # ruta donde se guardará la BBDD vectorizada\n",
    "\n",
    "# Se crea la BBDD de vectores a partir de los documentos y la función embeddings\n",
    "\n",
    "vector_store = SKLearnVectorStore.from_documents(\n",
    "    documents = documentos,\n",
    "    embedding = funcion_embedding,\n",
    "    persist_path = persist_path,\n",
    "    serializer = \"parquet\", # El serializador o formato de la BD lo definimos como parquet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17ca90b-ec2e-4bd9-a47e-d760f2637234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuerza a guardar los nuevos embeddins en disco\n",
    "vector_store.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eea918-2107-4156-9717-9996eea3931c",
   "metadata": {},
   "source": [
    "### Consulta del Usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080029f-6c86-48fe-91a8-1a5f2d67e4d0",
   "metadata": {},
   "source": [
    "- El usuario desea saber cual es el procedimiento para validar materias. Consultaremos en la base de datos lo que dice el reglamento y entregaremos esa información al LLM, para que construya una respuesta adecuada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7ce7ad-2a32-42b0-8558-78cc17923dc0",
   "metadata": {},
   "source": [
    "- **Pregunta del usuario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddcbddc-bd2e-407c-a760-e83779a12cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta_usuario = \"¿Qué procedimiento debo seguir para validar una materia en la universidad?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1b9397-9b49-420d-b20f-74d7608e4436",
   "metadata": {},
   "source": [
    "- **Consulta a la base de conocimiento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737db32a-8a40-4b8e-a1d8-314f1e17e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un nuevo documento que será nuestra \"consulta\" para buscar el de mayor similitud en nuestra BD\n",
    "\n",
    "consulta = \"Dame información validación de materias\"\n",
    "recuperados = vector_store.similarity_search(consulta)\n",
    "print(f\"CANTIDAD DE RECUPERADOS: {len(recuperados)}\\n ====== \\n\")\n",
    "print(recuperados[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d5cce-b13a-48af-88fa-119fdaab1cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexto = \"\\n\\n\".join([d.page_content for d in recuperados])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03f672-6c2b-4942-a042-6dd3def47e81",
   "metadata": {},
   "source": [
    "- **Construcción del prompt para enviar al LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8af86b-b77f-449a-af53-510f5dc9a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "plantilla = \"\"\"\n",
    "Responde la siguiente consulta de manera clara y completa **usando EXCLUSIVAMENTE la información proporcionada en los documentos recuperados**. \n",
    "Si la información no está en los documentos, responde indicando: \n",
    "\"No encontré información suficiente en los documentos recuperados.\"\n",
    "\n",
    "---\n",
    "Documentos recuperados:\n",
    "{contexto}\n",
    "\n",
    "Pregunta:\n",
    "{pregunta}\n",
    "\n",
    "Respuesta:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=plantilla,\n",
    "    input_variables=[\"contexto\", \"pregunta\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4391b-1f04-4d87-97a4-893189551004",
   "metadata": {},
   "source": [
    "- **estructuramos el prompt final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4884b-5535-4cf3-b99a-2cae39e83d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_final = prompt.format(contexto=contexto, pregunta=pregunta_usuario)\n",
    "#print(prompt_final)  # opcional, para ver cómo quedó\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6fa92-c0df-40ec-9b68-b25ae5586415",
   "metadata": {},
   "source": [
    "- **Lanzamos la pregunta al LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0972e0-6a49-4184-8f55-1b53ea8876e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = chat.invoke(prompt_final).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a73a39-10bb-4376-a5fe-ea011ddcb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respuesta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a9d85f-4ff9-4b93-a213-af50f050a7d6",
   "metadata": {},
   "source": [
    "## Cargar la base de datos de vectores (Uso posterior, una vez tenemos creada la BD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635df69d-8bdf-4be9-9ed4-541dc9400ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_connection = SKLearnVectorStore(\n",
    "    embedding = funcion_embedding, persist_path = persist_path, serializer = 'parquet' )\n",
    "\n",
    "print(\"Una instancia de la BBDD de vectores se ha cargado desde \",persist_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17c29f1-21b2-4e53-80cf-c61611c615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f12ee-81f9-4a3e-ad62-0761c7af17bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nueva_consulta = \"Buscar información sobre Trabajo Grado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef53f5-bec4-45eb-af5a-c2f2753d621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = vector_store_connection.similarity_search(nueva_consulta)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23053efb-129b-40a6-b3cc-d10b6a45074b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
