{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d0f526-b0ee-4203-9bce-bb7852807a73",
   "metadata": {},
   "source": [
    "# Conectar LangChain con modelos locales de Ollama y API's en Linea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df0528-12a7-4f8c-b07b-6ab1df009692",
   "metadata": {},
   "source": [
    "### Importante: \n",
    "- Previamente tener instalado langChain en el entorno de trabajo\n",
    "\n",
    " ```bash \n",
    " pip install langchain \n",
    " ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ece5391-98b7-44ea-828b-cdd18d1d4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6238c-b05a-457f-b797-375705ca1834",
   "metadata": {},
   "source": [
    "## Mensajes en LangChain (`SystemMessage` y `HumanMessage`)\n",
    "\n",
    "En LangChain, la comunicación con los modelos de lenguaje (LLMs) se basa en un sistema de mensajes que permiten estructurar las interacciones en un formato comprensible tanto para el modelo como para el desarrollador. Dos de los mensajes principales utilizados son `SystemMessage` y `HumanMessage`. A continuación, se explica qué son y para qué sirven, junto con ejemplos prácticos.\n",
    "\n",
    "---\n",
    "### 1. HumanMessage\n",
    "El HumanMessage representa una intervención realizada por un usuario humano. Es el mensaje que usualmente inicia la conversación o una nueva instrucción. Se utiliza para formular preguntas, dar comandos o realizar solicitudes al modelo.\n",
    "\n",
    "Propósito:\n",
    "- Representa lo que el usuario desea comunicar al modelo.\n",
    "-  Es el input principal que desencadena una respuesta del modelo\n",
    "\n",
    "\n",
    "#### **Ejemplo de uso:**\n",
    "```python\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "mensaje_usuario = HumanMessage(\n",
    "    content=\"¿Puedes explicarme cómo resolver una ecuación cuadrática paso a paso?\"\n",
    ")\n",
    "```\n",
    "\n",
    "### **2. `SystemMessage`**\n",
    "Un `SystemMessage` es un mensaje utilizado para definir el **contexto** o las **instrucciones iniciales** que guiarán al modelo durante toda la interacción. Se utiliza para establecer el comportamiento esperado del modelo o proporcionar un marco de trabajo para el resto de la conversación.\n",
    "\n",
    "#### **Propósito:**\n",
    "- Configurar el tono, los límites y las expectativas del modelo.\n",
    "- Asegurarse de que el modelo comprenda el propósito principal de la interacción.\n",
    "- Proveer instrucciones claras y precisas que influirán en cómo se interpretan y generan las respuestas.\n",
    "\n",
    "#### **Ejemplo de uso:**\n",
    "```python\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "mensaje_sistema = SystemMessage(\n",
    "    content=\"Eres un asistente experto en matemáticas que ayuda a resolver problemas paso a paso.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbc511-8a3c-4354-94fa-a651039c55ad",
   "metadata": {},
   "source": [
    "## Conectando con Ollama y sus modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b369c-836f-4b0b-9c04-068bd4cac768",
   "metadata": {},
   "source": [
    "- Podemos verificar en el siguiente bloque los modelos que tenemos instalados en Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c821d5c-6f44-4ff5-ba9d-eef6dd19ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME              ID              SIZE      MODIFIED   \n",
      "llama3.2:3b       a80c4f17acd5    2.0 GB    4 days ago    \n",
      "mistral:latest    6577803aa9a0    4.4 GB    4 days ago    \n",
      "gemma3:4b         a2af6cc3eb7f    3.3 GB    4 days ago    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Ejecuta el comando de Ollama y captura la salida\n",
    "result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109026af-9b8e-4a56-b875-35c726c3647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### importando las librerias requeridas para diferentes mensajes de LangChain\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adcc05f-537c-4c05-911c-1aed050a1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola! Estoy bien, gracias. ¿Y tú? ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"mistral:latest\")\n",
    "#chat = ChatOllama(model=\"gemma3:4b\")\n",
    "print(chat.invoke([HumanMessage(content=\"Hola, ¿cómo estás?\")]).content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af03d5f-6a1b-4c0c-b627-eec51fa228d6",
   "metadata": {},
   "source": [
    "## Conectando con OpenIA y sus modelos\n",
    "- Es necesario obtener el API KEY de OpenAI, siguiendo los pasos sugeridos en las diapositivas.  Simplemente accediendo a este link y haciendo la solicitud. Se reqiere tarjeta de crédito, y se tiene acceso gratuito por tiempo limitado: https://platform.openai.com/api-keys\n",
    "- Crear el arvhivo .env, con el api_key tal como se indica en las diapositivas.\n",
    "- Instalar en el prompt de anaconda:\n",
    "  \n",
    "``` bash \n",
    "pip install python-dotenv \n",
    "pip install langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d02a44ac-d45f-43c2-b84d-0c5df7c01bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectamos con OpenIA y se carga uno de sus modelos disponibles\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()  # Carga las variables desde .env\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5227c-5e7f-4d1f-8c39-71791d51ed5e",
   "metadata": {},
   "source": [
    "- Ahora podemos probar el mismo código anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f98786b1-8fae-454c-b199-056236a5c34f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'The project you are requesting does not allow user keys, please try again with a permissioned key.', 'type': 'invalid_request_error', 'param': None, 'code': 'not_authorized_invalid_key_type'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAuthenticationError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(chat.invoke([HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mHola, ¿cómo estás?\u001b[39m\u001b[33m\"\u001b[39m)]).content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    384\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    385\u001b[39m             stop=stop,\n\u001b[32m    386\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    387\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    388\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    389\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    390\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    391\u001b[39m             **kwargs,\n\u001b[32m    392\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate_with_cache(\n\u001b[32m    826\u001b[39m                 m,\n\u001b[32m    827\u001b[39m                 stop=stop,\n\u001b[32m    828\u001b[39m                 run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    829\u001b[39m                 **kwargs,\n\u001b[32m    830\u001b[39m             )\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m   1073\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1074\u001b[39m     )\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1177\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1175\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m     response = \u001b[38;5;28mself\u001b[39m.client.create(**payload)\n\u001b[32m   1178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1150\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1104\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1147\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1148\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1149\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1151\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1152\u001b[39m         body=maybe_transform(\n\u001b[32m   1153\u001b[39m             {\n\u001b[32m   1154\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1155\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1156\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1157\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1158\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1159\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1160\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1161\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1162\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1163\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1164\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1165\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1166\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1168\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1169\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1174\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1175\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1185\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1186\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1187\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1188\u001b[39m             },\n\u001b[32m   1189\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   1190\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1191\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   1192\u001b[39m         ),\n\u001b[32m   1193\u001b[39m         options=make_request_options(\n\u001b[32m   1194\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1195\u001b[39m         ),\n\u001b[32m   1196\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1197\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1198\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m   1199\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\lang-env\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mAuthenticationError\u001b[39m: Error code: 401 - {'error': {'message': 'The project you are requesting does not allow user keys, please try again with a permissioned key.', 'type': 'invalid_request_error', 'param': None, 'code': 'not_authorized_invalid_key_type'}}"
     ]
    }
   ],
   "source": [
    "print(chat.invoke([HumanMessage(content=\"Hola, ¿cómo estás?\")]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d621e-1fc4-488e-a852-e3bef988ac64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbca4b-bee5-4204-bf03-4086b92947f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
