{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59d0f526-b0ee-4203-9bce-bb7852807a73",
   "metadata": {},
   "source": [
    "# Conectar LangChain con modelos locales de Ollama y API's en Linea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df0528-12a7-4f8c-b07b-6ab1df009692",
   "metadata": {},
   "source": [
    "### Importante: \n",
    "- Previamente tener instalado langChain en el entorno de trabajo\n",
    "\n",
    " ```bash \n",
    " pip install langchain \n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3b8891-19e4-4263-89c0-b3dac40b502c",
   "metadata": {},
   "source": [
    "- Importar la librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ece5391-98b7-44ea-828b-cdd18d1d4c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbc511-8a3c-4354-94fa-a651039c55ad",
   "metadata": {},
   "source": [
    "## Conectando con Ollama y sus modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b369c-836f-4b0b-9c04-068bd4cac768",
   "metadata": {},
   "source": [
    "- Podemos verificar en el siguiente bloque los modelos que tenemos instalados en Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c821d5c-6f44-4ff5-ba9d-eef6dd19ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Ejecuta el comando de Ollama y captura la salida\n",
    "result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0028e67c-4de6-433a-ad39-91337e602894",
   "metadata": {},
   "source": [
    "- Se deben importar las clases para conectar e instanciar la conversacion con los modelos de Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109026af-9b8e-4a56-b875-35c726c3647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca924f81-311b-485b-bb4e-d28c770e0f9f",
   "metadata": {},
   "source": [
    "- Instanciar la conversación con alguno de los modelos de Ollama. Se utiliza el método `invoke` para realizar una solicitud al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcc05f-537c-4c05-911c-1aed050a1dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"deepseek-r1:8b\")\n",
    "#chat = ChatOllama(model=\"gemma3:4b\")\n",
    "print(chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\").content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af03d5f-6a1b-4c0c-b627-eec51fa228d6",
   "metadata": {},
   "source": [
    "## Conectando con OpenAI y sus modelos\n",
    "- Es necesario obtener el API KEY de OpenAI, siguiendo los pasos sugeridos en las diapositivas.  Simplemente accediendo a este link y haciendo la solicitud. Se requiere tarjeta de crédito, y se tiene acceso gratuito por tiempo limitado: https://platform.openai.com/api-keys\n",
    "- Crear el archivo .env, con el api_key tal como se indica en las diapositivas. El arhivo .env debe estar en el directorio del NoteBook.\n",
    "- Instalar en el prompt de anaconda:\n",
    "  \n",
    "``` bash \n",
    "pip install python-dotenv \n",
    "pip install langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b510c95-9516-4fce-83e2-f7040d0d2005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexion con OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env (busca automáticamente en el directorio actual o superiores)\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró OPENAI_API_KEY en el .env ni en el entorno\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9ff8c-08af-4a04-ab51-1f9c287476e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385121e-c0e4-499b-975a-b4ab9c73da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),   # sk-proj-...\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b2dc1a-e3b8-4e8f-9656-02338e677370",
   "metadata": {},
   "source": [
    "## Conectando con Google AI Studio y sus modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67222eb-29c4-4f71-9580-aa7cee2d7fe2",
   "metadata": {},
   "source": [
    "- Para conectar con los modelos Gemini de Google, se debe obtener una API_KEY de google. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5ca06-6b48-4571-b7c0-b70f7dd5ef6a",
   "metadata": {},
   "source": [
    "- Se deben instalar las librerias para conexion e instaniciación del chat\n",
    "\n",
    " ```bash \n",
    "pip install langchain-google-genai \n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce3c3d-6b2a-45e5-8fe7-82ded85eb661",
   "metadata": {},
   "source": [
    "- Se debe agregar el api_key como *GOOGLE_API_KEY* en el archivo .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fbca4b-bee5-4204-bf03-4086b92947f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conexión con Gemini (Google Generative AI)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# Cargar archivo .env\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "# Verificar que la API key esté disponible\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    raise ValueError(\"!! No se encontró GOOGLE_API_KEY en el .env ni en el entorno\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758ddcd-cd28-4d3c-8bd1-f6d47966fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear instancia del modelo\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    google_api_key=os.getenv(\"GOOGLE_API_KEY\"),   # AIza...\n",
    "    model=\"gemini-1.5-flash\",                     # puedes usar gemini-1.5-pro o gemini-1.0-pro también\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Probar llamada\n",
    "print(chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510ac6f2-b01c-4626-9e01-016c45b2e64d",
   "metadata": {},
   "source": [
    "## Conectando con modelos en linea de Groq\n",
    "- Groq es una plataforma en la nube que ofrece acceso a la api de varios modelos. No require de tarjeta para obtener api-key. Es gratuito para un número bajo de peticiones, ideal para uso educativo.\n",
    "- Ingresar a la web: https://groq.com/  loguearse, y obterner una api_key, agregarla al archivo .env como GROQ_API_KEY.\n",
    "- El siguiente código permite verificar la conexión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052afc39-1807-4f1d-97a8-be0e40db3c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Cargar la API key desde .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Crear conexión con Groq\n",
    "chat = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",   # consultar modelos en www.groq.com\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Probar conexión\n",
    "respuesta = chat.invoke(\"Hola, ¿cómo estás?, ¿quién eres?\")\n",
    "print(respuesta.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff7a612-675f-4356-9b9b-45d914e5328f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
