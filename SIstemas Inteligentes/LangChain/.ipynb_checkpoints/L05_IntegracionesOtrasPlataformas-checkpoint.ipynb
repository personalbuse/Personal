{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b590e18-2ca1-4d6c-8f9a-f4325b773bff",
   "metadata": {},
   "source": [
    "# Cargadores: Integración con documentos otras plataformas\n",
    "\n",
    "\n",
    "\r\n",
    "LangChain ofrece una variedad de cargadores de documentos, incluyendo aquellos que se integran con plataformas externas. Estos cargadores de tipo \"integración\" funcionan de forma similar a los cargadores estándar, pero están diseñados para conectarse directamente con fuentes de datos externas.\r\n",
    "\r\n",
    "### Ejemplos de integraciones disponibles:\r\n",
    "\r\n",
    "- Plataformas de terceros como Google Cloud, AWS, Google Drive o Dropbox.\r\n",
    "- Bases de datos como MongoDB.\r\n",
    "- Sitios web específicos, como Wikipedia.\r\n",
    "- Fuentes no convencionales como videos de YouTube o conversaciones de WhatsApp.\r\n",
    "\r\n",
    "Estas integraciones permiten cargar contenido directamente desde múltiples fuentes para luego ser procesado por LangChain, lo cual es útil en aplicaciones como sistemas de preguntas y respuestas basados en videos, análisis de conversaciones y más.\r\n",
    "\r\n",
    "### Documentación oficial\r\n",
    "\r\n",
    "Puedes consultar el listado completo de cargadores con integración en:\r\n",
    "\r\n",
    "https://python.langchain.com/v0.2/docs/integrations/document_loaders/\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e650a-f903-4343-8530-f21af14f7ed5",
   "metadata": {},
   "source": [
    "## Iniciamos creando el objeto LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae858a90-915c-434f-afa0-787e796339d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las clases para manejar conversaciones con modelos de Ollama\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "### Instaciamos un chat con uno de los modelos: llama3.2:3b, mistral:latest, gema3:4b, o los que se hayan instalado en Ollama\n",
    "\n",
    "chat = ChatOllama(model=\"llama3.2:3b\")\n",
    "#chat = ChatOllama(model=\"mistral:latest\")\n",
    "#chat = ChatOllama(model=\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e929914-d9ed-4c81-99a8-d3c086a59e87",
   "metadata": {},
   "source": [
    "## Integración con Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f039fb3-f081-4d66-911d-154603b582ed",
   "metadata": {},
   "source": [
    "### WikipediaLoader (LangChain)\n",
    "\n",
    "Un *document loader* que carga artículos de **Wikipedia** como objetos `Document`. Configurable por idioma, cantidad máxima, metadatos y tamaño del contenido. Ideal para integraciones en flujos RAG\n",
    "\n",
    "---\n",
    "\n",
    "#### Uso\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"Machine learning\", lang=\"es\", load_max_docs=2)\n",
    "docs = loader.load()\n",
    "```\n",
    "## Parámetros principales\n",
    "\n",
    "- **`query`** *(str)* → término de búsqueda en Wikipedia.  \n",
    "- **`lang`** *(str, default=\"en\")* → idioma de la búsqueda.  \n",
    "- **`load_max_docs`** *(int, default=100, límite 300)* → número máximo de artículos a cargar.  \n",
    "- **`load_all_available_meta`** *(bool, default=False)* → si se incluye toda la metadata disponible.  \n",
    "- **`doc_content_chars_max`** *(int, default=4000)* → límite de caracteres por documento.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d3412-7580-41c2-b407-a068ea57b539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wikipedia en una terminal\n",
    "from langchain.document_loaders import WikipediaLoader \n",
    "\n",
    "# Carguemos las demas clases requeridas de LangChain\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ead4f4-42fd-4110-a650-20127f609aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # obtener artículo de wikipedia\n",
    "personaje = \"Francisco de Paula Santander\"\n",
    "\n",
    "# Para ver los parámetros posibles en: https://python.langchain.com/v0.1/docs/integrations/document_loaders/wikipedia/\n",
    "docs = WikipediaLoader(query = personaje, lang = \"es\", load_max_docs = 10)  \n",
    "\n",
    "# para que sea más rápido solo pasamos el primer documento [0] como contexto extra\n",
    "contexto_extra = docs.load()[0].page_content \n",
    "    \n",
    "# pregunta usuario\n",
    "human_prompt = HumanMessagePromptTemplate.from_template('Responde a esta pregunta\\n{pregunta}, aqui tienes contenido extra:\\n{contenido}')\n",
    "\n",
    "# Construir prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])\n",
    "\n",
    "# Resultado\n",
    "pregunta_arg = \"¿Dame una breve biografía del personaje?\"\n",
    "result = chat.invoke(chat_prompt.format_prompt(pregunta = pregunta_arg, contenido = contexto_extra).to_messages())\n",
    "    \n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133ea87-d137-4293-9867-d4232f23eb2e",
   "metadata": {},
   "source": [
    "- **Veamos el contexto extra que ha tenido el LLM como base:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ee9d8-d0c1-4cd8-bb76-18c7edaa47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contexto_extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b944e8df-ce71-4520-99e0-aa1724691e34",
   "metadata": {},
   "source": [
    "\n",
    "El **`ArxivLoader`** es un *document loader* de LangChain que permite buscar y cargar artículos científicos desde **arXiv**.  \n",
    "\n",
    "- **Búsqueda**: acepta consultas por palabra clave, categorías (`cs.AI`, `stat.ML`, etc.) o ID de paper.  \n",
    "- **Salida**: devuelve una lista de objetos `Document`.  \n",
    "  - `page_content`: texto del paper (abstract o PDF completo si está disponible).  \n",
    "  - `metadata`: incluye título, autores, fecha de publicación, categorías, URL del PDF y el **abstract** oficial (`Summary`).  \n",
    "- **Parámetros clave**:  \n",
    "  - `query` → término de búsqueda.  \n",
    "  - `load_max_docs` → número máximo de resultados.  \n",
    "\n",
    "Ideal para integrar papers en flujos de **RAG** o para análisis de literatura científica en proyectos de investigación.\n",
    "\n",
    "El manual de usuario de la API de ArXiv aqui: https://info.arxiv.org/help/api/user-manual.html\n",
    "El manual de cargador aqui: https://python.langchain.com/docs/integrations/document_loaders/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858a53d7-7642-4de2-9f8d-677378b30dec",
   "metadata": {},
   "source": [
    "### Ejemplo:\n",
    "- Buscar 3 papers en arXiv y mostrar la metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9797fa2c-8754-43d9-b018-d7f27b358bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Instalar por consola: \n",
    "# pip install  arxiv\n",
    "# pip install pymupdf\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# 1) Configura tu búsqueda (palabras clave o categorías de arXiv, p. ej. \"cs.AI\")\n",
    "query = \"cs.AI and transformers for time series\"\n",
    "n_results = 3\n",
    "\n",
    "# 2) Carga resultados desde arXiv (abstracts + metadatos)\n",
    "loader = ArxivLoader(query=query, load_max_docs=n_results)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6044dba6-95bc-4ffb-84f4-f7d3c6adae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Published': '2023-10-30', 'Title': 'DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection', 'Authors': 'Chaocheng Yang, Tingyin Wang, Xuanhui Yan', 'Summary': 'Anomaly detection in multivariate time series has emerged as a crucial\\nchallenge in time series research, with significant research implications in\\nvarious fields such as fraud detection, fault diagnosis, and system state\\nestimation. Reconstruction-based models have shown promising potential in\\nrecent years for detecting anomalies in time series data. However, due to the\\nrapid increase in data scale and dimensionality, the issues of noise and Weak\\nIdentity Mapping (WIM) during time series reconstruction have become\\nincreasingly pronounced. To address this, we introduce a novel Adaptive Dynamic\\nNeighbor Mask (ADNM) mechanism and integrate it with the Transformer and\\nDenoising Diffusion Model, creating a new framework for multivariate time\\nseries anomaly detection, named Denoising Diffusion Mask Transformer (DDMT).\\nThe ADNM module is introduced to mitigate information leakage between input and\\noutput features during data reconstruction, thereby alleviating the problem of\\nWIM during reconstruction. The Denoising Diffusion Transformer (DDT) employs\\nthe Transformer as an internal neural network structure for Denoising Diffusion\\nModel. It learns the stepwise generation process of time series data to model\\nthe probability distribution of the data, capturing normal data patterns and\\nprogressively restoring time series data by removing noise, resulting in a\\nclear recovery of anomalies. To the best of our knowledge, this is the first\\nmodel that combines Denoising Diffusion Model and the Transformer for\\nmultivariate time series anomaly detection. Experimental evaluations were\\nconducted on five publicly available multivariate time series anomaly detection\\ndatasets. The results demonstrate that the model effectively identifies\\nanomalies in time series data, achieving state-of-the-art performance in\\nanomaly detection.'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "608d402f-1a70-47bf-ab5a-a5df54268492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Documento 1 ===\n",
      "Título : DDMT: Denoising Diffusion Mask Transformer Models for Multivariate Time Series Anomaly Detection\n",
      "Autores: Chaocheng Yang, Tingyin Wang, Xuanhui Yan\n",
      "Año    : 2023\n",
      "\n",
      "Abstract:\n",
      "Anomaly detection in multivariate time series has emerged as a crucial challenge in time series\n",
      "research, with significant research implications in various fields such as fraud detection, fault\n",
      "diagnosis, and system state estimation. Reconstruction-based models have shown promising potential\n",
      "in recent years for detecting anomalies in time series data. However, due to the rapid increase in\n",
      "data scale and dimensionality, the issues of noise and Weak Identity Mapping (WIM) during time\n",
      "series reconstruction have become increasingly pronounced. To address this, we introduce a novel\n",
      "Adaptive Dynamic Neighbor Mask (ADNM) mechanism and integrate it with the Transformer and Denoising\n",
      "Diffusion Model, creating a new framework for multivariate time series anomaly detection, named\n",
      "Denoising Diffusion Mask Transformer (DDMT). The ADNM module is introduced to mitigate information\n",
      "leakage between input and output features during data reconstruction, thereby alleviating the\n",
      "problem of WIM during reconstruction. The Denoising Diffusion Transformer (DDT) employs the\n",
      "Transformer as an internal neural network structure for Denoising Diffusion Model. It learns the\n",
      "stepwise generation process of time series data to model the probability distribution of the data,\n",
      "capturing normal data patterns and progressively restoring time series data by removing noise,\n",
      "resulting in a clear recovery of anomalies. To the best of our knowledge, this is the first model\n",
      "that combines Denoising Diffusion Model and the Transformer for multivariate time series anomaly\n",
      "detection. Experimental evaluations were conducted on five publicly available multivariate time\n",
      "series anomaly detection datasets. The results demonstrate that the model effectively identifies\n",
      "anomalies in time series data, achieving state-of-the-art performance in anomaly detection.\n",
      "\n",
      "=== Documento 2 ===\n",
      "Título : Transformers in Time Series: A Survey\n",
      "Autores: Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, Liang Sun\n",
      "Año    : 2023\n",
      "\n",
      "Abstract:\n",
      "Transformers have achieved superior performances in many tasks in natural language processing and\n",
      "computer vision, which also triggered great interest in the time series community. Among multiple\n",
      "advantages of Transformers, the ability to capture long-range dependencies and interactions is\n",
      "especially attractive for time series modeling, leading to exciting progress in various time series\n",
      "applications. In this paper, we systematically review Transformer schemes for time series modeling\n",
      "by highlighting their strengths as well as limitations. In particular, we examine the development of\n",
      "time series Transformers in two perspectives. From the perspective of network structure, we\n",
      "summarize the adaptations and modifications that have been made to Transformers in order to\n",
      "accommodate the challenges in time series analysis. From the perspective of applications, we\n",
      "categorize time series Transformers based on common tasks including forecasting, anomaly detection,\n",
      "and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend\n",
      "decomposition analysis to study how Transformers perform in time series. Finally, we discuss and\n",
      "suggest future directions to provide useful research guidance. To the best of our knowledge, this\n",
      "paper is the first work to comprehensively and systematically summarize the recent advances of\n",
      "Transformers for modeling time series data. We hope this survey will ignite further research\n",
      "interests in time series Transformers.\n",
      "\n",
      "=== Documento 3 ===\n",
      "Título : Dateformer: Time-modeling Transformer for Longer-term Series Forecasting\n",
      "Autores: Julong Young, Junhui Chen, Feihu Huang, Jian Peng\n",
      "Año    : 2023\n",
      "\n",
      "Abstract:\n",
      "Transformers have demonstrated impressive strength in long-term series forecasting. Existing\n",
      "prediction research mostly focused on mapping past short sub-series (lookback window) to future\n",
      "series (forecast window). The longer training dataset time series will be discarded, once training\n",
      "is completed. Models can merely rely on lookback window information for inference, which impedes\n",
      "models from analyzing time series from a global perspective. And these windows used by Transformers\n",
      "are quite narrow because they must model each time-step therein. Under this point-wise processing\n",
      "style, broadening windows will rapidly exhaust their model capacity. This, for fine-grained time\n",
      "series, leads to a bottleneck in information input and prediction output, which is mortal to long-\n",
      "term series forecasting. To overcome the barrier, we propose a brand-new methodology to utilize\n",
      "Transformer for time series forecasting. Specifically, we split time series into patches by day and\n",
      "reform point-wise to patch-wise processing, which considerably enhances the information input and\n",
      "output of Transformers. To further help models leverage the whole training set's global information\n",
      "during inference, we distill the information, store it in time representations, and replace series\n",
      "with time representations as the main modeling entities. Our designed time-modeling Transformer --\n",
      "Dateformer yields state-of-the-art accuracy on 7 real-world datasets with a 33.6\\% relative\n",
      "improvement and extends the maximum forecast range to half-year.\n"
     ]
    }
   ],
   "source": [
    "# Requiere: pip install langchain langchain-community arxiv pymupdf \n",
    "\n",
    "import textwrap  # Para dar formato a cadenas de texto largas\n",
    "\n",
    "def limpia(texto: str) -> str:\n",
    "    # Quita saltos de línea raros y espacios dobles\n",
    "    return \" \".join(texto.split())\n",
    "\n",
    "for i, d in enumerate(docs[:n_results], start=1):\n",
    "    m = d.metadata or {}\n",
    "    titulo = m.get(\"Title\", \"N/A\")\n",
    "    autores = m.get(\"Authors\", \"N/A\")\n",
    "    anio = (m.get(\"Published\") or \"N/A\")[:4]\n",
    "    abstract = m.get(\"Summary\") or d.page_content  # prefiero el abstract oficial\n",
    "\n",
    "    print(f\"\\n=== Documento {i} ===\")\n",
    "    print(f\"Título : {titulo}\")\n",
    "    print(f\"Autores: {autores}\")\n",
    "    print(f\"Año    : {anio}\")\n",
    "    print(\"\\nAbstract:\")\n",
    "    print(textwrap.fill(limpia(abstract), width=100))  # Escribe lineas de maximo 100 caracteres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee73fdd-2d4d-46ba-9e86-51b8cc81740f",
   "metadata": {},
   "source": [
    "# Transformación de documentos\n",
    "\n",
    "\r\n",
    "Una vez que se carga un documento desde una fuente externa mediante un cargador, se obtiene un objeto de tipo `Document`, cuyo contenido principal se encuentra en el campo `page_content`.\r\n",
    "\r\n",
    "### División en fragmentos (chunks)\r\n",
    "\r\n",
    "En muchos casos, el texto dentro de `page_content` puede ser demasiado extenso para ser procesado directamente por un modelo de lenguaje, ya que estos modelos suelen tener un límite en la cantidad de tokens que pueden manejar (por ejemplo, unos 8,000 tokens, equivalentes a unas 6,000 palabras aproximadamente).\r\n",
    "\r\n",
    "Para resolver este problema, LangChain proporciona transformadores de documentos. Estos permiten dividir el contenido de `page_content` en fragmentos más pequeños, llamados *chunks*, de manera automática y controlada.\r\n",
    "\r\n",
    "### Utilidad de los fragmentos\r\n",
    "\r\n",
    "Estos fragmentos tienen múltiples usos. Uno de los más importantes es la conversión de cada fragmento en un vector numérico mediante un proceso de incrustación (*embedding*). Estos vectores pueden almacenarse y luego utilizarse para realizar búsquedas eficientes basadas en similitud.\r\n",
    "\r\n",
    "Por ejemplo, si estamos construyendo una aplicación de preguntas y respuestas basada en documentos, los vectores de cada fragmento permitirán encontrar rápidamente el contenido más relevante para responder una consulta, sin necesidad de recorrer todo el contenido de manera secuencial.\r\n",
    "\r\n",
    "Esta estrategia mejora significativamente la eficiencia y precisión cuando se necesita proporcionar contexto adicional a un modelo de lenguaje grande (LLM).\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af707f-b79a-423d-8150-cd47fa9cc648",
   "metadata": {},
   "source": [
    "## Cargar Fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7e322b3-e5a2-4532-89f6-c5bcb923c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Datos/ReglamentoEstudiantil.txt', encoding= \"utf8\") as file:\n",
    "    texto_completo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e575affc-2eed-4348-9bb6-ba771fc8b84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116473"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de caracteres\n",
    "len(texto_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82825491-fd2d-4279-ac1d-1e1f21aecc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17235"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numero de palabras\n",
    "len(texto_completo.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735401c-6ea0-4478-b329-c2215fd7641d",
   "metadata": {},
   "source": [
    "## Transformador \"CharacterTextSplitter\"\n",
    "\r\n",
    "\r\n",
    "Uno de los transformadores más utilizados en LangChain para dividir texto en fragmentos es `CharacterTextSplitter`. Este componente permite separar el contenido de un documento (`page_content`) en bloques más pequeños basados en un número específico de caracteres.\r\n",
    "\r\n",
    "Esta división es útil cuando el texto original es demasiado largo para ser procesado por completo por un modelo de lenguaje, ya que permite mantener fragmentos manejables en tamaño, sin perder el orden ni el contexto genercon LLMs.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e2b6ba-41e5-4ed2-9369-36c19309c779",
   "metadata": {},
   "source": [
    "#  Transformadores de documentos en LangChain\n",
    "\n",
    "| Categoría                     | Clase / Ejemplo                               | Uso principal                                                                 |\n",
    "|--------------------------------|-----------------------------------------------|-------------------------------------------------------------------------------|\n",
    "| **Text Splitters**            | `CharacterTextSplitter`                      | Divide texto por separadores simples (ej. saltos de línea, espacios).          |\n",
    "|                                | `RecursiveCharacterTextSplitter`              | Divide jerárquicamente (párrafos → frases → palabras), el más recomendado.     |\n",
    "|                                | `TokenTextSplitter`                          | Divide en chunks según número de *tokens* (ej. compatible con OpenAI).        |\n",
    "|                                | `MarkdownTextSplitter`                       | Divide preservando la estructura de encabezados Markdown.                      |\n",
    "|                                | `LatexTextSplitter`                          | Especializado en documentos LaTeX.                                            |\n",
    "|                                | `HTMLHeaderTextSplitter`                     | Divide respetando encabezados HTML (h1, h2, etc.).                            |\n",
    "|                                | `PythonCodeTextSplitter`, `JavascriptSplitter` | Segmentación de código por funciones, clases o bloques.                        |\n",
    "| **Metadata & Filters**        | `EmbeddingsRedundantFilter`                  | Elimina chunks redundantes usando similitud de embeddings.                     |\n",
    "|                                | `EmbeddingsClusteringFilter`                 | Agrupa chunks en *clusters* semánticos.                                       |\n",
    "|                                | `EmbeddingsFilter`                           | Filtra documentos según relevancia semántica.                                 |\n",
    "|                                | `LLMChainFilter`                             | Usa un LLM para decidir qué documentos conservar.                             |\n",
    "| **Document Compressors**      | `DocumentCompressorPipeline`                 | Encadena pasos de compresión y filtrado de documentos.                        |\n",
    "|                                | `LLMChainExtractor`                          | Usa un LLM para extraer información clave y reducir texto.                    |\n",
    "| **Summarization Chains**      | `StuffDocumentsChain`                        | Une todos los documentos en un solo prompt.                                   |\n",
    "|                                | `MapReduceDocumentsChain`                    | Resumen en dos fases: map (local) y reduce (global).                          |\n",
    "|                                | `RefineDocumentsChain`                       | Construye resúmenes incrementales, refinando cada paso.                       |\n",
    "| **Custom Transformers**       | `BaseDocumentTransformer` (clase base)       | Crear tus propios transformadores: limpieza, normalización, extracción, etc.  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7924d1fe-d6e7-43f2-a2d8-e859d98896c2",
   "metadata": {},
   "source": [
    "## Ejemplo de aplicación con `CharacterTextSplitter`\n",
    "\n",
    "`CharacterTextSplitter` permite ajustar parámetros como:\n",
    "- `chunk_size`: la longitud máxima de cada fragmento.\n",
    "- `chunk_overlap`: la cantidad de caracteres que se solapan entre fragmentos consecutivos (útil para preservar contexto entre divisiones).\n",
    "\n",
    "Es una herramienta simple pero poderosa para preparar documentos antes de realizar tareas como incrustación (embeddings), búsqueda semántica o consultas con LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d6ecec8-1866-4353-b104-f637f5c1d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cc0faeec-db86-4b19-b5f7-b780994dbe85",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Se indica que se divida en párrafos de tamaño de alrededor de 1000 caracteres\n",
    "text_splitter = CharacterTextSplitter(separator = '\\n', chunk_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "46dc3789-018a-467f-b1dc-b0703d719d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# se crea el documento utilizando el transformador\n",
    "textos = text_splitter.create_documents([texto_completo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1e9d99d7-c1ec-4a18-8145-e80ac92cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "\n",
      "<class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "\n",
      "page_content='b. Admisión: Resultado del proceso de selección de los aspirantes, según los \n",
      "criterios de selección vigentes y políticas que la Universidad emita para tal \n",
      "fin. \n",
      "c. Matrícula: Acto que realiza una persona voluntariamente para inscribirse en \n",
      "un programa académico, que involucra registro de la información personal, \n",
      "matrícula financiera y matrícula académica. \n",
      "\f",
      "\n",
      "Acuerdo No.186 del 02 de diciembre de 2005 \n",
      "d. \n",
      "e. \n",
      "Homologación: Reconocimiento por las autoridades académicas de \n",
      "contenidos y créditos cursados y aprobados, ó intensidad horaria, en un \n",
      "programa académico debidamente reconocido por el MEN y registrado ante el \n",
      "ICFES, o su equivalente en el exterior, de esta u otra Institución de Educación \n",
      "Superior. \n",
      "Validación de asignaturas: Mecanismo mediante el cual se podrá tener por \n",
      "cumplidas las exigencias académicas de una asignatura teórica que no haya \n",
      "sido cursada. \n",
      "f. \n",
      "g. \n",
      "Cancelación: Anulación total o parcial del registro de las asignaturas,'\n"
     ]
    }
   ],
   "source": [
    "print(type(textos)) # Verficamos el tipo del objeto obtenido\n",
    "print('\\n')\n",
    "print(type(textos[0]))  # Verificamos el tipo de cada elemento\n",
    "print('\\n')\n",
    "print(textos[2])\n",
    "#print(textwrap.fill(textos[2].page_content, width=80))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "490af4f9-5173-43af-aee7-e938d5b97813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "964"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textos[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b2d77ad6-b732-4e1e-a19c-2662ed1045ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='4. Que se deben ajustar las normas y procedimientos a las políticas de cobertura \n",
      "social establecidas y a la retención de estudiantes y mejoramiento de la calidad de \n",
      "la educación. \n",
      "5. Que es necesario, establecer y aplicar en el Reglamento Académico Estudiantil, \n",
      "los principios de equidad y justicia, consagrados en la Constitución Nacional. \n",
      "6. Que el propósito del presente Acuerdo, es tener un documento único que consigne \n",
      "todas las modificaciones efectuadas y propuestas. \n",
      "ACUERDA: \n",
      "CAPÍTULO I. GENERALIDADES \n",
      "ARTÍCULO 1.-Definiciones. \n",
      "a. Inscripción: Proceso de registro de la información personal y académica de \n",
      "los solicitantes a programas que la Universidad de Pamplona ofrece en las \n",
      "diferentes modalidades, para adquirir la calidad de aspirante. \n",
      "b. Admisión: Resultado del proceso de selección de los aspirantes, según los \n",
      "criterios de selección vigentes y políticas que la Universidad emita para tal \n",
      "fin.'\n"
     ]
    }
   ],
   "source": [
    "print(textos[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "33f3d7f0-0eeb-4698-93d0-bac5852ca725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textos[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18802fca-920a-48e8-8b35-3cff39f6aff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4. Que se deben ajustar las normas y procedimientos a las políticas de cobertura \\nsocial establecidas y a la retención de estudiantes y mejoramiento de la calidad de \\nla educación. \\n5. Que es necesario, establecer y aplicar en el Reglamento Académico Estudiantil, \\nlos principios de equidad y justicia, consagrados en la Constitución Nacional. \\n6. Que el propósito del presente Acuerdo, es tener un documento único que consigne \\ntodas las modificaciones efectuadas y propuestas. \\nACUERDA: \\nCAPÍTULO I. GENERALIDADES \\nARTÍCULO 1.-Definiciones. \\na. Inscripción: Proceso de registro de la información personal y académica de \\nlos solicitantes a programas que la Universidad de Pamplona ofrece en las \\ndiferentes modalidades, para adquirir la calidad de aspirante. \\nb. Admisión: Resultado del proceso de selección de los aspirantes, según los \\ncriterios de selección vigentes y políticas que la Universidad emita para tal \\nfin.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textos[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a23bdf-a0f1-4385-ba18-a6a4c058faea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lang-env)",
   "language": "python",
   "name": "lang-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
